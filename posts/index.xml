<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Eulogs</title>
    <link>uildDrafts/posts/</link>
    <description>Recent content in Posts on Eulogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 30 Apr 2023 20:26:02 +0530</lastBuildDate><atom:link href="uildDrafts/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Autoregressive Models: Connecting the Dots</title>
      <link>uildDrafts/posts/autoregressive-models-connecting-the-dots/</link>
      <pubDate>Sun, 30 Apr 2023 20:26:02 +0530</pubDate>
      
      <guid>uildDrafts/posts/autoregressive-models-connecting-the-dots/</guid>
      <description>Desc Text.</description>
      <content:encoded><![CDATA[<style>
div {
  margin-bottom: 15px;
  padding: 4px 12px;
}
.Example {
    background-color: #DCDCDC;
    border-left: 6px solid #696969;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Theoram {
        background-color: #E9FFDB;
        border-left: 6px solid #4C9A2A;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Definition {
        background-color: #e1f1fd;
        border-left: 6px solid #72A0C1;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Important{
        background-color: #FFFFE0;
        border-left:6px solid #FFDF00;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Problem {
    background-color: #ffdddd;
    border-left: 6px solid #f44336;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Info {
    background-color: #e7f3fe;
    border-left: 6px solid #2196F3;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Question {
    background-color: #FAF0E6;
    border-left: 6px solid #DEB887;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
</style>
<blockquote>
<p>The future will soon be a thing of past</p>
<p><cite>&ndash;George Carlin</cite></p>
</blockquote>
<p>Blogpost will talk about <em>need</em> and <em>what</em> of Autoregressive models, but before going towards the <em>what</em>, its&rsquo; better to first understand why we felt the need to have another type of models in machine learning community. We will look at the Motivation behind the creation and what problems we want to solve using them.</p>
<h2 id="motivation">Motivation</h2>
<h3 id="problems-we-would-like-to-solve">Problems we would like to solve</h3>
<ol>
<li><strong>Generating Data:</strong> syntheizing images, text, videos, speech</li>
<li><strong>Compressing data:</strong> constructing efficient codes.</li>
<li><strong>Anomaly Detection:</strong> e.g. supervised estimator is forced to make a decision even over the datapoint it is uncertain about</li>
</ol>
<p>Suppose there exist true data distribution \(p_{data}(\textbf{x})\), now you most probably will not be able to know this distribution but what you can do is sample some data points from it \(\textbf{x}^{(1)}, \textbf{x}^{(2)} &hellip; \textbf{x}^{(n)} \sim p_{data}(\textbf{x})\). Now likelihood based models try to estimate this \(p_{data}\) by finding a distribution that closely fits to the datapoints sampled. But likelihood function we try to find should basically find probability \(p(\textbf{x})\) for any arbitrary \(\textbf{x}\) such that it will actually closely fits to true distribution and not just sampled distribution, and also be able to sample \(\textbf{x} \sim p(\textbf{x})\)</p>
<p>Its&rsquo; important to note that just because we can sample from distribution so we can compute it too or just because we can compute the distribution then we can sample from it too. But using Autoregressive models we will be able to do both, and actually computing probability will be lot easier than generating samples.</p>
<h3 id="desiderata">Desiderata</h3>
<ol>
<li>
<p>We want to estimate distribution of complex high dimenional data, lets&rsquo; say you have an image of \(128\times 128\times 3\) which will correspond to \(\sim \textbf{50,000}\) pixels(or dimensions). And its&rsquo; just not for one image datapoint but whole lot of dataset.</p>
</li>
<li>
<p>To learn such high dimensional data we need computationl and statistical efficiency. Think of <em>computational efficiency</em> as operation should not take much time and <em>statistical efficiency</em> as it should not take whole lot of datapoints to start understanding the patterns in the datapoints with whatever model we use.</p>
<ul>
<li>Efficient training(computationally and statistically) and model representation</li>
<li>Expressiveness and Generalization</li>
<li>Sampling quality and speed</li>
<li>Compression rate and speed</li>
</ul>
</li>
</ol>
<h2 id="function-approximation">Function Approximation</h2>
<p>Our goal is to estimate true distribution \(p_{data}\) using sampled datapoints \(\textbf{x}^{(1)}, \textbf{x}^{(2)} &hellip; \textbf{x}^{(n)} \sim p_{data}(\textbf{x})\). So we introduce <strong>function approximation</strong> which will learn the weights/parameters \(\theta\) of the likelihood function(here, Neural Network) such that \(p_{\theta}(\textbf{x}) \approx p_{data}(\textbf{x})\).</p>
<div class="Question">
   <strong>‚ùìQuestion</strong></br>
      <ol>
      	<li>
	   How do we design such function approximators to effectively represent complex <b>joint distributions</b> over <b>x</b>, yet remain easy to train?
	</li>
      </ol>
</div>
<p>Now, when we train neural nets what we are actually doing is finding the best weights for the model in order to find best likelihood function. So, designing the model and training procedure go hand in hand. And thi pose a search problem over the parameters.
$$\arg \min_{\theta} \textbf{loss}(\theta, \textbf{x}^{(1)}, &hellip; ,\textbf{x}^{(n)})$$
For intutive understanding, think of likelihood function to be <em>Gaussian Distribution</em> and \(\theta\) be its mean and variance. So, while training \(\theta\) will keep updating in order to fit some distribution.</p>
<p>Most likely objective we use is <em>Maximum Likelihood</em>
$$\arg \min_{\theta} loss(\theta, \textbf{x}^{(1)}, &hellip;, \textbf{x}^{(n)}) = \frac{1}{n} \sum_{i=1}^{n}-\log p_{\theta}(\textbf{x}^{(i)})$$
It is interesting to note that minimizing KL Divergence is asymptotically equivalent to maximizing the likelihood.</p>
<div class="Question">
   <strong>‚ùìHow minimizing KL Divergence asymptotically equivalent to maximizing the likelihood?</strong></br>
   <p>Let there exist likelihood function for true distribution <span
class="math inline">\(P(x\vert\theta^{*})\)</span> and approximate
function <span class="math inline">\(P(x\vert\theta)\)</span> <span
class="math display">\[\begin{aligned}
\theta_{\min KL} &amp;= \arg
\min_{\theta}D_{KL}\left[P(x\vert\theta^{*})\Vert P(x\vert\theta)\right]
\\
                  &amp;= \arg \min_{\theta} \mathbb{E}_{x \sim P(x \vert
\theta^{*})}\left[\log
\frac{P(x\vert\theta^{*})}{P(x\vert\theta)}\right]
\end{aligned}\]</span>  <span
class="math inline">\(P(x\vert\theta^{*})\)</span> is not going to
contribute to the optimization and can be popped out of equation  <span
class="math display">\[\begin{aligned}
\theta_{\min KL} &amp;= \arg \min_{\theta} \mathbb{E}_{x \sim P(x \vert
\theta^{*})}\left[-\log P(x\vert\theta)\right] \\
                  &amp;= \arg \max_{\theta} \mathbb{E}_{x \sim P(x \vert
\theta^{*})}\left[\log P(x\vert\theta)\right]
\end{aligned}\]</span>  Applying the law of large numbers, <span
class="math display">\[\begin{aligned}
\theta_{\min KL} &amp;= \arg \max_{\theta} \lim_{n \rightarrow \infty}
\frac{1}{n}\sum_{i=1}^{n} \log P(x_{i}\vert \theta) \\
                  &amp;= \arg \max_{\theta} \log P(x \vert \theta) \\
                  &amp;= \arg \max_{\theta} P(x \vert \theta) \\
                  &amp;= \theta_{max MLE}
\end{aligned}\]</span></p>
</div>
<p>Okay, back to our search problem over parameters we found loss function as maximum likelihood but what procedure to follow for optimization? <strong>Stochastic Gradient Descent</strong> (SGD) seems to work just fine!</p>
<h2 id="setting-bayes-as-base">Setting Bayes as base</h2>
<p>Let us a consider a high-dimensional random variable \(\textbf{x} \in \mathcal{X}^{D}\) where \(\mathcal{X} = {0, 1, &hellip;, 255}\) (e.g. pixel values) or \(\mathcal{X}=\mathbb{R}\). Our goal is to model \(p(\textbf{x})\). For intutive undertanding, say you have an image whose height and width are same as \(\sqrt{D}\) which implies there would be \(D\) pixels or dimensions, each dimension can vary over \(\mathbb{R}\). So to model image \(\textbf{x}\) would be equivalent to model the joint distribution of all the pixels or dimensions of the image.</p>
<p>$$
\begin{aligned}
p(x) &amp;= p(x_{1}, x_{2},&hellip;, x_{d}) \cr
&amp;= p(x_{1})p(x_{2}\vert x_{1})&hellip;p(x_{D}\vert x_{1},&hellip;, x_{D-1}) \cr
&amp;= p(x_{1})\prod_{d=2}^{D}p(x_{d}\vert x_{&lt;d}) \cr
\log p(x) &amp;= \boxed{p(x_{1})\sum_{d=2}^{D}\log p(x_{d}\vert x_{&lt;d})}
\end{aligned}
$$</p>
<p>This is <strong>Autoregressive Model</strong>.</p>
<p>Modelling all conditional distribution separately is simplt infeasible! If we did that we would obtain \(D\) different models, and complexity of each model will grow to varying conditioning. Can we solve this issue? <strong>Yes</strong>, by using single, shared model for the all the conditional distributions and that can be done using neural network as that model.</p>
<h2 id="deep-autoregressive-models-architectures">Deep Autoregressive Models Architectures</h2>
<p>Reearch community has mostly focused on improving the network architectures of deep autoregressive models [4], some areas they focused on are as follows:-</p>
<ul>
<li><strong>Increasing receptive field and memory</strong>: This ensured the network has access to all parts of the input to encourage consistency</li>
<li><strong>Increasing network capacity</strong>: Which allowed more complex distributions to be modelled.</li>
</ul>
<figure>
    <img loading="lazy" src="/AutoregressiveModelling/taxonomy.png"
         alt="Taxonomy of Autoregressive Models" width="800px" height="450px"/> <figcaption>
            <p>Taxonomy of Autoregressive Models</p>
        </figcaption>
</figure>

<p>In the above figure we have tried to do as many plausible classifications of Autoregressive models, there can be more but in the community these are the important ones.</p>
<p>We are no going to learn any of those methods in this blogpost. In future, links to published blogs corresponding to those methods will be added here as list and you can access them directly. This will ease up the navigation process then may it be you want to access root(like in trees) blog post of <a href="https://www.eulogs.com/posts/generative-modelling/#why-generative-modelling">Generative Modelling: Stormbreaker ü™ì in AI world</a> series or any leaf blog post like LSTM.</p>
<h2 id="citation">Citation</h2>
<p>Cited as:</p>
<blockquote>
<p>Garg, P. (2023, May 2). Autoregressive Models: Connecting the Dots. Eulogs. Retrieved May 2, 2023, from <a href="https://www.eulogs.com/posts/autoregressive-models-connecting-the-dots/">https://www.eulogs.com/posts/autoregressive-models-connecting-the-dots/</a></p>
</blockquote>
<p>or</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bibtex" data-lang="bibtex"><span class="line"><span class="cl"><span class="nc">@article</span><span class="p">{</span><span class="nl">garg_2023</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">   <span class="na">title</span>   <span class="p">=</span> <span class="s">&#34;Autoregressive Models: Connecting the dots&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">author</span>  <span class="p">=</span> <span class="s">&#34;Garg, Priyam&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">journal</span> <span class="p">=</span> <span class="s">&#34;www.eulogs.com&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">year</span>    <span class="p">=</span> <span class="s">&#34;2023&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">month</span>   <span class="p">=</span> <span class="s">&#34;May&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">url</span>     <span class="p">=</span> <span class="s">&#34;https://www.eulogs.com/posts/autoregressive-models-connecting-the-dots/&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="references">References</h2>
<p>[1] Pieter Abbeel. (2020, February 5). L2 Autoregressive Models &ndash; CS294-158-SP20 Deep Unsupervised Learning &ndash; UC Berkeley, Spring 2020 [Video]. YouTube. <a href="https://www.youtube.com/watch?v=iyEOk8KCRUw">https://www.youtube.com/watch?v=iyEOk8KCRUw</a></p>
<p>[2] Tae, J. (2020a, March 9). Jake Tae. MLE and KL Divergence. <a href="https://jaketae.github.io/study/kl-mle/">https://jaketae.github.io/study/kl-mle/</a></p>
<p>[3] Tomczak, J. M. (2022). Deep Generative Modeling. Springer Nature</p>
<p>[4] Bond-Taylor, S., Leach, A., Long, Y., &amp; Willcocks, C. G. (2021). Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11), 7327‚Äì7347. <a href="https://doi.org/10.1109/tpami.2021.3116668">https://doi.org/10.1109/tpami.2021.3116668</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Generative Modelling: Stormbreaker ü™ì in AI world</title>
      <link>uildDrafts/posts/generative-modelling/</link>
      <pubDate>Fri, 28 Apr 2023 07:45:46 +0530</pubDate>
      
      <guid>uildDrafts/posts/generative-modelling/</guid>
      <description>Transformers, Diffusion models, GANs and many such models have taken the world by storm. They all come under the umbrella term of Generative Modelling. In this blog we will see vastness and limitless usage of these models along with their taxonomy.</description>
      <content:encoded><![CDATA[<style>
div {
  margin-bottom: 15px;
  padding: 4px 12px;
}
.Example {
    background-color: #DCDCDC;
    border-left: 6px solid #696969;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Theoram {
	background-color: #E9FFDB;
	border-left: 6px solid #4C9A2A;
	color: #41424C;
	box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Definition {
	background-color: #e1f1fd;
	border-left: 6px solid #72A0C1;
	color: #41424C;
	box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Important{
	background-color: #FFFFE0;
	border-left:6px solid #FFDF00;
	color: #41424C;
	box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Problem {
    background-color: #ffdddd;
    border-left: 6px solid #f44336;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Info {
    background-color: #e7f3fe;
    border-left: 6px solid #2196F3;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
</style>
<p>Through this blogpost we are going to gain in-depth understanding of Generative Models, why we need them, along with current taxanomy. But before diving into the <em>Why?</em>, <em>What?</em> of Generative Models, let us consider a simple example of trained Deep Neural Network(DNN) that classifies images of animals. Although the network is trained so well but adding noise to image could output false classification, even though noise added to image doesn&rsquo;t change the semantics under human perception. This example indicates that neural networks that are used to parameterize the conditional distribution \(p(y|\textbf{x})\) seem to lack <em>semantic</em> understanding of images. A useful model in real world will not only output probability of correctness of classification for example but will be able to tell its level of uncertainity for the answer.</p>
<h2 id="why-generative-modelling">Why Generative Modelling?</h2>
<p>Deep Learning models cannot go on making decision about the problems without understanding the reality, and express uncertainity about surrounding world. Assume we have some two-dimensional data and new data-point to be classified. Now, there are two approaches we can use to solve this problem. First, classifier could estimate the result by modelling conditional distribution \(p(y|\textbf{x})\). Second, we can consider joint distribution \(p(\textbf{x}, y)\) that could be further decomposed as \(p(\textbf{x}, y)=p(y|\textbf{x})p(\textbf{x})\).</p>
<figure>
    <img loading="lazy" src="/GenerativeModelling/Discriminative_Generative.png"
         alt="And example of data (left) and two approaches to decision making: (middle) a discriminative approach and (right) a generative approach"/> <figcaption>
            <p>And example of data (left) and two approaches to decision making: (middle) a discriminative approach and (right) a generative approach</p>
        </figcaption>
</figure>

<p>In the above figure, both the approaches i.e. discriminative and generative have made a decision boundary. On one hand former is quite certain of &ldquo;X&rdquo; being a part of blue region whereas latter uses \(p(\textbf{x})\) to account for additional understanding. Now, &ldquo;X&rdquo; lies far from orange zone, but also does not lie in the region of high probability mass in blue zone due to which generative approach said \(p(\textbf{x})\) as low. And, if we want these models to be communicative enough such that as humans we can understand why thy are making decisions that they are making, that would be a <strong>crucial</strong> skill to exploit.</p>
<h3 id="significance-of-span-classmath-inlineempememxemspan">Significance of <span class="math inline"><em>p</em>(<em>x</em>)</span></h3>
<p>From the <em>generative</em> perspective, knowing the distribution \(p(\textbf{x})\) is essential because:</p>
<ul>
<li>It could be used to assess whether a given object has been observed in past or not.</li>
<li>It could help to properly weight the decision.</li>
<li>It could be used to assess uncertainity about the environment.</li>
<li>It could be used to actively learn by interacting with environment( e.g. by asking for labeling objects with low \(p(\textbf{x})\))</li>
<li>It could be used to generate (syntheize) new objects.</li>
</ul>
<p>Though, some if not most of the readers might be ignorant to the use of \(p(\textbf{x})\) only as generator of new data, but above points bring into varied perspectives of it. And at this point, the sail that once started from island of discriminative modelling brought us to the vast land of generative modelling.</p>
<h2 id="where-can-we-use-deep-generative-modelling">Where Can we use (Deep) Generative Modelling?</h2>
<p>Deep Generative Modelling need high computational power for training and with the advent of GPU and Deep Learning Frameworks like PyTorch, TensorFlow etc we have seen vast applications thereof.</p>
<p>Some applications vary from typical modalities considered in machine learning:-</p>
<ol>
<li><strong>Text Analysis:</strong> Question-Answering, Machine Translation, Text-to-Image Generation</li>
<li><strong>Image-Analysis:</strong> Data Augmentation, Super Resolution, Image Inpainting, Image Denoising, Object Transfiguration, Image Colorization, Image Captioning, Video Prediction etc.</li>
<li><strong>Audio Analysis:</strong> WaveNet</li>
<li><strong>Active Learning:</strong> Generate synthetic medical images, generate synthetic text data etc.</li>
<li><strong>Reinforcement Learning:</strong> World Models</li>
<li><strong>Graph Analysis:</strong> Understanding Interaction Dynamics in Social Networks, Anomaly Detection, Protein Structure Modelling, Source Code Generation, Semantic Parsing.</li>
</ol>
<h2 id="how-to-formulate-deep-generative-modelling">How to Formulate (Deep) Generative Modelling?</h2>
<p>After asking some important questions of <em>What</em> and <em>where</em>, its&rsquo; time to ask <em>how</em>. In other words we can understand the section heading as how to express \(p(\textbf{x})\). We can divide (Deep) Generative modelling into four categories:- \((1)\) Autoregressive Generative Models(ARM) \((2)\) Flow-based Models \((3)\) Latent variable models \((4)\) Energy-based models</p>
<figure>
    <img loading="lazy" src="/GenerativeModelling/taxonomy.png"
         alt="A taxonomy of deep generative models" width="700px" height="550px"/> <figcaption>
            <p>A taxonomy of deep generative models</p>
        </figcaption>
</figure>

<p>Deep Generative models can be divided into two types of density estimation methods, namely \((1)\) Explicit Density Models \((2)\) Implicit Density Models. Say you have data distribution \(p_{data}\), \(p_{model}(\textbf{x};\theta)\) likelihood function which learns that data distribution and \(\textbf{x}=[x_{1}, x_{2}, &hellip;, x_{n}]^{T}\) are data-points of dataset. Now in explicit density models we compute \(p_{model}(\textbf{x};\theta)\) in short \(p(\textbf{x})\), but in implicit models we dont&rsquo; need to compute this likelihood function but directly sample from the distribution e.g. sampling values from noise, and generate for instance image samples using some transformation.</p>
<table>
<thead>
<tr>
<th style="text-align:center">Explicit Generative Methods</th>
<th style="text-align:center">Impliit Generative Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">define an explicit density form(\(p_{model}(\textbf{x};\theta)\)) that allows likelihood inference</td>
<td style="text-align:center">target a flexible transformation(Generator) from random noise to generated samples</td>
</tr>
<tr>
<td style="text-align:center">used to estimate the probability density function of the data</td>
<td style="text-align:center">can generate new samples from the learned distribution</td>
</tr>
<tr>
<td style="text-align:center">require computing the likelihood of the data, which can be computationally expensive</td>
<td style="text-align:center">do not require computing the likelihood</td>
</tr>
<tr>
<td style="text-align:center">typically easier to interpret and analyze</td>
<td style="text-align:center">more flexible and can capture complex distributions</td>
</tr>
<tr>
<td style="text-align:center">can suffer from overfitting</td>
<td style="text-align:center">can suffer from mode collapse</td>
</tr>
<tr>
<td style="text-align:center">trained using maximum likelihood estimation</td>
<td style="text-align:center">typically trained using adversarial training or variational inference</td>
</tr>
</tbody>
</table>
<h3 id="explicit-generative-methods">Explicit Generative Methods</h3>
<p>Explicit density methods need to estimate the density of underlying data distribution and marginal likelihood in denominator of bayes rule \(p(z|x)=\frac{p(x,z)}{\int p(x|z)p(z)}\) creates problem since it involves integral. Now if this likelihood can be solved in closed-form expression then those problems lie under <em>tractable density</em> otherwise we need to find approximations to it and so those problems lie under <em>approximate density</em> class. Also, tractable density can be computed in polynomial time but intractable(not approximate) takes more than exponential time to solve. In the given link of <a href="https://en.wikipedia.org/wiki/Closed-form_expression">Closed Form Expressions</a> a table is given where you can check that we do not have any closed form solution under integral.</p>
<h4 id="tractable-generative-models">Tractable Generative Models</h4>
<p>In tractable density models we have two type of techniques namely \((1)\) Autoregressive Models \((2)\) Flow-based Models.</p>
<h5 id="autoregressive-models">Autoregressive Models</h5>
<p>Here, distribution over \(\textbf{x}\) is represented in an autoregressive manner:</p>
<p>$$p(\textbf{x})=p(x_{0})\prod_{i=1}^{D}p(x_{i}\vert\textbf{x}_{&lt;i})$$</p>
<p>where, \(\textbf{x}_{&lt;i}\) denotes all \(\textbf{x}\)&rsquo;s upto index \(i\).</p>
<p>Modelling all conditional distributions \(p(x_{i}\vert\textbf{x}_{&lt;i})\) would be computationally inefficient. But there are many ways we solve this, although we will not talk about autoregessive models here. Checkout the post <a href="https://www.eulogs.com/posts/autoregressive-models-connecting-the-dots/">Autoregressive Models: Connecting the dots</a> that talk about why we needed these type of models anyway along with its taxonomy.</p>
<h5 id="flow-based-models">Flow-based Models</h5>
<p>The change of variable formula provides a principled manner of expressing a density of a random variable by transforming it with an invertible transformation \(f\).</p>
<p>$$p(\textbf{x})=p(z=f(x))\vert\text{J}_{f(\textbf{x})}\vert$$</p>
<p>where \(\text{J}_{f(\textbf{x})}\) denotes the Jacobian Matrix.</p>
<p>We can parameterize \(f\) using deep neural networks; however, it cannot be any arbitrary neural networks, because we must be able to calculate the Jacobian matrix. All generative models that take advantage of change of variables formula are referred as <strong>flow-based models</strong> or <em>flows</em> for short.</p>
<h4 id="approximate-generative-models">Approximate Generative Models</h4>
<p>In approximate density models we have two type of techniques namely \((1)\) Prescribed Models \((2)\) Energy-based Models.</p>
<h5 id="prescribed-models">Prescribed Models</h5>
<p>The idea behind <strong>latent variable models</strong> is to assume a lower-dimensional latent space and the following generative process:
$$\textbf{z} \sim p(\textbf{z})$$
$$\textbf{x} \sim p(\textbf{x}\vert\textbf{z})$$
Latent variables corresponds to hidden factor in data, and the conditional distribution \(p(\textbf{x}\vert\textbf{z})\) could be treated as a <em>generator</em>.</p>
<p>The most widely known known latent variable model is <strong>probabilisitc Principal Component Analysis</strong> (pPCA) where \(p(\textbf{z})\) and \(p(\textbf{x}\vert\textbf{z})\) are Gaussian distibutions, and dependency between \(\textbf{z}\) and \(\textbf{x}\) is linear.</p>
<p>A non-linear extension of pPCA with arbitratry distributions is the <strong>Variational Auto-Encoder</strong> (VAE) framework. In VAEs and the pPCA all distributions must be defined upfront and, therefore, they are called <em>prescribed models</em>.</p>
<h5 id="energy-based-models">Energy Based Models</h5>
<p>Physics provide an interesting perspective on defining a group of generative models through defining an <em>energy function</em>, \(E(\textbf{x})\), and, eventually the Boltzmann distribution:
$$p(\textbf{x}) = \frac{e^{-E(\textbf{x})}}{Z}$$
where \(Z=\sum_{\textbf{x}}e^{-E(\textbf{x})}\) is the partition function. \(Z\) normalizes the values of function.</p>
<p>The main idea behind EBMs is to formulate the energy function and calculate(or rather approximate) the partition function.</p>
<h3 id="implicit-generative-methods">Implicit Generative Methods</h3>
<h4 id="gans">GANs</h4>
<p>Above all described methods except EBMs used <em>log-likelihood</em> function for density estimation, but another approach uses <em>adversarial loss</em> where discriminator \(D(\cdot)\) determines a difference between real data and synthetic data provided by the generator in the implicit form, these are <strong>Generative Adversaial Networks</strong>(GANs) under <em>implicit models</em>.</p>
<div class="Important">
‚ùó<strong>Important</strong></br>
      It is to be noted that these groups dont' create hard demarcations and there are methods that use concepts from more than one groups for e.g. flow-based GAN model
</div>
<h2 id="overview">Overview</h2>
<p>Below is provided a table that shows comparision of all the four groups of methods on some arbitrary criteria like:-</p>
<ul>
<li>Whether training is typically <strong>stable</strong>?</li>
<li>Whether it is possible to calculate <strong>likelihood function</strong>?</li>
<li>Whether one can use a model for <strong>lossy</strong> or <strong>lossless</strong> compression?</li>
<li>Whether a model can be used for <strong>Representation Learning</strong>?</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">Generative Models</th>
<th style="text-align:center">Training</th>
<th style="text-align:center">Likelihood</th>
<th style="text-align:center">Sampling</th>
<th style="text-align:center">Compression</th>
<th style="text-align:center">Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Autoregressive</td>
<td style="text-align:center"><span style="color:#03AC13">Stable</span></td>
<td style="text-align:center"><span style="color:#03AC13">Exact</span></td>
<td style="text-align:center"><span style="color:#B90E0A">Slow</span></td>
<td style="text-align:center"><span style="color:#03AC13">Lossless</span></td>
<td style="text-align:center"><span style="color:#B90E0A">No</span></td>
</tr>
<tr>
<td style="text-align:left">Flow-based</td>
<td style="text-align:center"><span style="color:#03AC13">Stable</span></td>
<td style="text-align:center"><span style="color:#03AC13">Exact</span></td>
<td style="text-align:center"><span style="color:#03AC13">Fast</span>/<span style="color:#B90E0A">Slow</span></td>
<td style="text-align:center"><span style="color:#03AC13">Lossless</span></td>
<td style="text-align:center"><span style="color:#03AC13">Yes</span></td>
</tr>
<tr>
<td style="text-align:left">Implicit</td>
<td style="text-align:center"><span style="color:#B90E0A">Unstable</span></td>
<td style="text-align:center"><span style="color:#B90E0A">No</span></td>
<td style="text-align:center"><span style="color:#03AC13">Fast</span></td>
<td style="text-align:center">No</td>
<td style="text-align:center"><span style="color:#B90E0A">No</span></td>
</tr>
<tr>
<td style="text-align:left">Prescribed</td>
<td style="text-align:center"><span style="color:#03AC13">Stable</span></td>
<td style="text-align:center"><span style="color:#03AC13">Approximate</span></td>
<td style="text-align:center"><span style="color:#03AC13">Fast</span></td>
<td style="text-align:center"><span style="color:#B90E0A">Lossy</span></td>
<td style="text-align:center"><span style="color:#03AC13">Yes</span></td>
</tr>
<tr>
<td style="text-align:left">Energy-based</td>
<td style="text-align:center"><span style="color:#03AC13">Stable</span></td>
<td style="text-align:center"><span style="color:#B90E0A">Unnormalized</span></td>
<td style="text-align:center"><span style="color:#B90E0A">Slow</span></td>
<td style="text-align:center">Rather not</td>
<td style="text-align:center"><span style="color:#03AC13">Yes</span></td>
</tr>
</tbody>
</table>
<h2 id="citation">Citation</h2>
<blockquote>
<p>Garg, P. (2023a, April 30). Generative Modelling: Stormbreaker ü™ì in AI world. Eulogs. Retrieved May 2, 2023, from <a href="https://www.eulogs.com/posts/generative-modelling/">https://www.eulogs.com/posts/generative-modelling/</a></p>
</blockquote>
<p>or</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bibtex" data-lang="bibtex"><span class="line"><span class="cl"><span class="nc">@article</span><span class="p">{</span><span class="nl">garg_2023</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">title</span>   <span class="p">=</span> <span class="s">&#34;Generative Modelling: Stormbreaker ü™ì in AI world&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">author</span>  <span class="p">=</span> <span class="s">&#34;Garg, Priyam&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">journal</span> <span class="p">=</span> <span class="s">&#34;www.eulogs.com&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">year</span>    <span class="p">=</span> <span class="s">&#34;2023&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">month</span>   <span class="p">=</span> <span class="s">&#34;Apr&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">url</span>     <span class="p">=</span> <span class="s">&#34;https://www.eulogs.com/posts/generative-modelling/&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="references">References</h2>
<p>[1] Tomczak, J. M. (2022). Deep Generative Modeling. Springer Nature</p>
<p>[2] Wu, Q., Gao, R., &amp; Zha, H. (2021). Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators. In Neural Information Processing Systems (Vol. 34). <a href="https://papers.nips.cc/paper/2021/hash/5db60c98209913790e4fcce4597ee37c-Abstract.html">https://papers.nips.cc/paper/2021/hash/5db60c98209913790e4fcce4597ee37c-Abstract.html</a></p>
<p>[3] Bond-Taylor, S., Leach, A., Long, Y., &amp; Willcocks, C. G. (2021). Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11), 7327‚Äì7347. <a href="https://doi.org/10.1109/tpami.2021.3116668">https://doi.org/10.1109/tpami.2021.3116668</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Condition Number in Matrix</title>
      <link>uildDrafts/posts/condition_number/</link>
      <pubDate>Sat, 15 Apr 2023 13:59:40 +0530</pubDate>
      
      <guid>uildDrafts/posts/condition_number/</guid>
      <description>Brief overview of Condition Number and Matrix Norm along with examples</description>
      <content:encoded><![CDATA[<style>
div {
  margin-bottom: 15px;
  padding: 4px 12px;
}
.Example {
    background-color: #DCDCDC;
    border-left: 6px solid #696969;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Theoram {
        background-color: #E9FFDB;
        border-left: 6px solid #4C9A2A;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Definition {
        background-color: #e1f1fd;
        border-left: 6px solid #72A0C1;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Important{
        background-color: #FFFFE0;
        border-left:6px solid #FFDF00;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Problem {
    background-color: #ffdddd;
    border-left: 6px solid #f44336;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Info {
    background-color: #e7f3fe;
    border-left: 6px solid #2196F3;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
</style>
<h2 id="condition-number">Condition Number</h2>
<p>A <em>condition number</em> measures how sensitive is the solution of equation(s) to perturbations in the input is.</p>
<h2 id="introduction">Introduction</h2>
<figure>
    <img loading="lazy" src="/ConditionNumber/SolSpace.png" width="600px" height="200px"/> 
</figure>

<p>Consider a system of linear equations in the form of \(Ax=b\), where we are interested to find \(x\). So we have a solution space of \(x \in R^{n}\) for \(b \in R^{m}\) while unchanging \(A\).</p>
<p>Now our objective is to quantify the change in solution space of \(x\) if we add some perturbations or error term to input space of \(b\). As we can see in the above figure that altering the input space will give us $$A(x+\Delta{x})=b+\Delta{b}$$</p>
<p>Te reach our objective, we first need to understand what are matrix norms.</p>
<h2 id="matrix-norms">Matrix Norms</h2>
<p>The norm of square matrix \(A\) is non-negative real number denoted \(\lVert A \rVert\). It is a way of determining the &ldquo;size&rdquo; of a matrix that is not necessarily related to how many rows or columns the matrix has. There are various different ways of defining a matrix norm.</p>
<h3 id="1-norm">1-Norm</h3>
<p>$$\lVert A \rVert_{1} = \max_{1 \le \ j \ \le n}(\sum_{i=1}^{n}|a_{ij}|)$$</p>
<p>We sum the absolute values down each column and then take the max value out of those.</p>
<figure>
    <img loading="lazy" src="/ConditionNumber/OneNorm.png" height="150px"/> 
</figure>

<h3 id="infinity-norm">Infinity-Norm</h3>
<p>$$\lVert A \rVert_{\infty} = \max_{1 \le \ i \ \le n}(\sum_{j=1}^{n}|a_{ij}|)$$</p>
<p>We sum the absolute values along each row and then take the max out of those.</p>
<figure>
    <img loading="lazy" src="/ConditionNumber/InfinityNorm.png" height="140px"/> 
</figure>

<p>We have taken square matrix \(A\) but same formulas can be applied for rectangular matrices too.</p>
<h3 id="properties">Properties</h3>
<ol>
<li>\(\lVert A \rVert\) \(\ge\) 0 for any square matrix</li>
<li>\(\lVert A \rVert\) \(=\) 0 <em>iff</em> \(A=0\)</li>
<li>\(\lVert kA \rVert\) = |k|\(\lVert A \rVert\), for any scalar \(k\)</li>
<li>\(\lVert A + B \rVert \le \lVert A \rVert + \lVert B \rVert\)</li>
<li>\(\lVert AB \rVert \le \lVert A \rVert \lVert B \rVert\)</li>
</ol>
<h2 id="condition-number-1">Condition Number</h2>
<p>Matrix norm is enough for us to start working on condition number. We will start by accounting in \(Ax=b\) and \(A(x+\Delta{x})=b+\Delta{b}\).</p>
<p>$$
\begin{aligned}
\cancel{Ax} &amp;= \cancel{b} \cr
\cancel{Ax}+A\Delta{x} &amp;= \cancel{b} + \Delta{b} \cr
\hline \cr
A\Delta{x} &amp;= \Delta{b} \cr
\hline
\end{aligned}
$$</p>
<p>applying the \(5^{th}\) property of matrix norms to \(Ax=b\)</p>
<p>$$
\begin{aligned}
\lVert b \rVert = \lVert Ax \rVert &amp;\le \lVert A \rVert \lVert x \rVert \cr
\frac{1}{\lVert x \rVert} &amp;\le \frac{\lVert A \rVert}{\lVert b \rVert}
\end{aligned}
$$</p>
<p>multiplying \(A\Delta{x} = \Delta{b}\) by \(A^{-1}\) on both sides</p>
<p>$$
\begin{aligned}
A^{-1}A\Delta{x} &amp;= A^{-1}\Delta{b} \cr
I\Delta{x} &amp;= A^{-1}\Delta{b}
\end{aligned}
$$
taking matrix norm on both sides
$$\lVert \Delta{x} \rVert = \lVert A^{-1}\Delta{b} \rVert \le \lVert A^{-1} \rVert \lVert \Delta{b} \rVert$$</p>
<p>we get,
$$\boxed{\frac{\lVert \Delta{x} \rVert}{\lVert x \rVert} \le \lVert A \rVert \lVert A^{-1} \rVert \frac{\lVert \Delta{b} \rVert}{\lVert b \rVert}}$$
This says that the upper bound in relative error of \(x\) would be scaled to \(\kappa(A) = \lVert A \rVert \lVert A^{-1} \rVert\) multiplied by relative error in \(b\), where \(\kappa(A)\) is <strong>Condition Number</strong>. Calculating \(\lVert A \rVert\) is relatively easy than finding \(\lVert A^{-1} \rVert\).</p>
<p>\(\Delta{b}\) is usually generated by rounding error programs, or when we made some approximation mistake. Hence, it is quite small and this implies relative error in \(x\) depends on condition number. Also we generally use infinity-norm to find matrix norms of matrices.</p>
<div class="Example"> 
  <strong>üìì Example</strong></br>
  Consider
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>7</mn><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding="application/x-tex">5x_{1} + 7x_{2} = 0.7</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>10</mn><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">7x_{1} + 10x_{2} = 1</annotation></semantics></math>
<p><span style="color:#92282E;">Solution</span> x = [0, 0.1]</p>
<p>Perturbed system <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>7</mn><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mn>0.69</mn></mrow><annotation encoding="application/x-tex">5x_{1} + 7x_{2} = 0.69</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>10</mn><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mn>1.01</mn></mrow><annotation encoding="application/x-tex">7x_{1} + 10x_{2} = 1.01</annotation></semantics></math><br>
has the <span style="color:#92282E;">solution</span> x&rsquo; = [-0.17, 0.22]</p>
<p>The relative error in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
is given by,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mo stretchy="false" form="prefix">‚à•</mo><mi>Œî</mi><mi>x</mi><mo stretchy="false" form="postfix">‚à•</mo></mrow><mrow><mo stretchy="false" form="prefix">‚à•</mo><mi>x</mi><mo stretchy="false" form="postfix">‚à•</mo></mrow></mfrac><mo>=</mo><mn>1.7</mn></mrow><annotation encoding="application/x-tex">\frac{\lVert \Delta{x} \rVert}{\lVert x \rVert} = 1.7</annotation></semantics></math></p></p>
<p>and the relative error in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
is given by,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mo stretchy="false" form="prefix">‚à•</mo><mi>Œî</mi><mi>b</mi><mo stretchy="false" form="postfix">‚à•</mo></mrow><mrow><mo stretchy="false" form="prefix">‚à•</mo><mi>b</mi><mo stretchy="false" form="postfix">‚à•</mo></mrow></mfrac><mo>=</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">\frac{\lVert \Delta{b} \rVert}{\lVert b \rVert} = 0.01</annotation></semantics></math></p>
<p>now 1&#37 error in input is giving a 170&#37 error in output which is significantly large.</p>
<p>Let‚Äôs check the condition number using the infinity norm.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∫</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>289</mn></mrow><annotation encoding="application/x-tex">\kappa(A) = 289</annotation></semantics></math></p>
<p>This says that the upper bound of relative error in solution space is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>289</mn><mi>x</mi><mn>0.01</mn><mo>=</mo><mn>2.89</mn></mrow><annotation encoding="application/x-tex">289x0.01=2.89</annotation></semantics></math></p>
</div>
<p>A matrix with large condition number is said to be <strong>ill conditioned</strong>. Whereas a matrix with small condition matrix is said to be <strong>well conditioned</strong>. Although large and small is hard to quantify since it depends on many factors of which some can be how powerful computing power we working with, what type of applications we are working with etc.</p>
<div class="Theoram">
	<strong>üìè Theoram</strong></br>
	Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
be a square non-singular matrix then for any singular matrix of the same
size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>,
we have
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><mi>Œ∫</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>‚â§</mo><mfrac><mrow><mo stretchy="false" form="prefix">‚à•</mo><mi>A</mi><mo>‚àí</mo><mi>B</mi><mo stretchy="false" form="postfix">‚à•</mo></mrow><mrow><mo stretchy="false" form="prefix">‚à•</mo><mi>A</mi><mo stretchy="false" form="postfix">‚à•</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{\kappa(A)} \le \frac{\lVert A - B\rVert}{\lVert A \rVert}</annotation></semantics></math>
<p>This shows that if you have singular matrix which is quite close to non-singular matrix then it will decrease the relative error but inversely decrease the condition number too. In other words, closer the singular matrix to non-singular matrix higher the <em>condition number</em>.</p>
<div class="Example">
	<strong>üìì Example</strong></br>
	Consider a matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">B=\begin{pmatrix} 1 &amp; 1 \\
1 &amp; 1 \end{pmatrix}</annotation></semantics></math> which is
singularly non-invertible since its determinant is 0.<br />
For any
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œµ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon \gt 0</annotation></semantics></math>,
let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn><mo>+</mo><mi>œµ</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn><mo>‚àí</mo><mi>œµ</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A=\begin{pmatrix} 1 &amp; 1+\epsilon \\
1-\epsilon &amp; 1 \end{pmatrix}</annotation></semantics></math> then,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mo>‚àí</mo><mn>1</mn></mrow></msup><mo>=</mo><msup><mi>œµ</mi><mrow><mo>‚àí</mo><mn>2</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mo>‚àí</mo><mn>1</mn><mo>‚àí</mo><mi>œµ</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mo>‚àí</mo><mn>1</mn><mo>+</mo><mi>œµ</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A^{-1} = \epsilon^{-2}\begin{pmatrix} 1 &amp; -1-\epsilon \\ -1 + \epsilon &amp; 1 \end{pmatrix}</annotation></semantics></math>
Taking matrix norms,<br />
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">‚à•</mo><mi>A</mi><msub><mo stretchy="false" form="postfix">‚à•</mo><mi>‚àû</mi></msub><mo>=</mo><mn>2</mn><mo>+</mo><mi>œµ</mi></mrow><annotation encoding="application/x-tex">\lVert A \rVert_{\infty} = 2+\epsilon</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">‚à•</mo><msup><mi>A</mi><mrow><mo>‚àí</mo><mn>1</mn></mrow></msup><msub><mo stretchy="false" form="postfix">‚à•</mo><mi>‚àû</mi></msub><mo>=</mo><msup><mi>œµ</mi><mrow><mo>‚àí</mo><mn>2</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>+</mo><mi>œµ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lVert A^{-1} \rVert_{\infty} = \epsilon^{-2}(2+\epsilon)</annotation></semantics></math><br />
then,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∫</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="prefix">‚à•</mo><mi>A</mi><msub><mo stretchy="false" form="postfix">‚à•</mo><mi>‚àû</mi></msub><mo stretchy="false" form="prefix">‚à•</mo><msup><mi>A</mi><mrow><mo>‚àí</mo><mn>1</mn></mrow></msup><msub><mo stretchy="false" form="postfix">‚à•</mo><mi>‚àû</mi></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>2</mn><mi>œµ</mi></mfrac><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>&gt;</mo><mfrac><mn>4</mn><msup><mi>œµ</mi><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">\kappa(A)=\lVert A \rVert_{\infty}\lVert A^{-1} \rVert_{\infty}= (\frac{2}{\epsilon} + 1)^{2} &gt; \frac{4}{\epsilon^{2}}</annotation></semantics></math>
let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œµ</mi><mo>‚â§</mo><mn>0.01</mn><mo>‚üπ</mo><mi>Œ∫</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>40</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">\epsilon \le 0.01 \implies \kappa(A)=40,000</annotation></semantics></math><br />
As
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œµ</mi><mo>‚Üí</mo><mn>0</mn><mo>,</mo><mi>A</mi><mo>‚Üí</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">\epsilon \rightarrow 0, A \rightarrow B</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∫</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚Üí</mo><mi>‚àû</mi></mrow><annotation encoding="application/x-tex">\kappa(A) \rightarrow \infty</annotation></semantics></math>.
This will result in very sensitive computation since
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∫</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚Üí</mo><mi>‚àû</mi></mrow><annotation encoding="application/x-tex">\kappa(A) \rightarrow \infty</annotation></semantics></math>
</div>
</div>
<h2 id="references">References</h2>
<p>[1] <a href="https://nucinkis-lab.cc.ic.ac.uk/HELM/workbooks/workbook_30/30_4_matrx_norms.pdf">Matrix norms 30 - Imperial College London</a></p>
<p>[2] Prof. S. Baskar, &ldquo;Week 3 : Lecture 16 : Matrix Norms: Condition Number of a Matrix.&rdquo;, YouTube, uploaded by IIT Bombay July 2018, 02 February 2023, <a href="https://www.youtube.com/watch?v=IXNz7BU0CDc">https://www.youtube.com/watch?v=IXNz7BU0CDc</a></p>
<p>[3] Robert van de Geijn, &ldquo;1 4 1 The condition number of a matrix&rdquo;, YouTube, uploaded by Advanced LAFF, 17 September 2019, <a href="https://www.youtube.com/watch?v=CGfXxLnnHtg&amp;t=379s">https://www.youtube.com/watch?v=CGfXxLnnHtg&amp;t=379s</a></p>
<p>[4] Jones, A. (2020, May 17). Condition numbers. Andy Jones. Retrieved April 15, 2023, from <a href="https://andrewcharlesjones.github.io/journal/condition-number.html">https://andrewcharlesjones.github.io/journal/condition-number.html</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Shortcut Learning: Belief trap of Deep Neural Networks</title>
      <link>uildDrafts/posts/shortcut-learning/</link>
      <pubDate>Fri, 31 Mar 2023 23:13:25 +0530</pubDate>
      
      <guid>uildDrafts/posts/shortcut-learning/</guid>
      <description>Is your neural network really learning what you intend it to learn? Pressing attention of researchers around the world is required to tackle the common enemy of shortcut learning</description>
      <content:encoded><![CDATA[<style>
div {
  margin-bottom: 15px;
  padding: 4px 12px;
}
.Example {
    background-color: #DCDCDC;
    border-left: 6px solid #696969;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Theoram {
        background-color: #E9FFDB;
        border-left: 6px solid #4C9A2A;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Definition {
        background-color: #e1f1fd;
        border-left: 6px solid #72A0C1;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Important{
        background-color: #FFFFE0;
        border-left:6px solid #FFDF00;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Problem {
    background-color: #ffdddd;
    border-left: 6px solid #f44336;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Info {
    background-color: #e7f3fe;
    border-left: 6px solid #2196F3;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
</style>
<blockquote>
<p>Life is not complex. We are complex. Life is simple, and the simple thing is the right thing.</p>
<p>-<cite>Oscar Wilde</Cite></p>
</blockquote>
<p>Over the span of last decade, Deep Learning has made our life simpler by bringing its power into the applications of healthcare, self-driving cars, fraud-prevention and many more. Since the launch of ChatGPT many companies have made applications which help users in brainstorming and simplifying comlpex ideas, assist in writing content, even assist in writing codes etc. Their predictive analysis power depends on the assumption that these models are learning what we intend them to learn, e.g. to classify a cow, model must be looking at cow and not at grass. But has it been the case the whole time?</p>
<h2 id="introduction">Introduction</h2>
<p>Lets&rsquo; start with an example of Bob and Alice who are studying for high school exams. Bob used a strategy of mugging up while Alice likes to understand things in detail and then ponder over it. Bobs&rsquo; strategy got him good grades in test unlike Alice, but can Bob extrapolate/generalize this strategy in order to apply the learned concepts to real world applications like Alice. Alice can use her strategy almost anywhere in life and would benefit from it but Bob will fail in the wild. Bob has taken shortcut to complete the task instead of actually learning principles. But are these shortcuts bad? When human driver takes shortcut route to reach destination its good for you since it saved time. So it depends on the type of task we are doing and also the context of it to determine taking shortcut is okay or not.</p>
<p>We are using Deep Learning techniques especially for the field of computer vision in self drving cars, security cameras, activity recognition etc on daily basis. But even after 5 decades we haven&rsquo;t really understood the underlying principles behind the success of these models. Only on the basis of test performances we are deploying them in the wild. Since they are being used as applications in court, defence, person identification etc which makes the need to understand them necessity than leaving it as another field of scientific endeavour[<a href="#1">1</a>].</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"><figure>
    <img loading="lazy" src="/ShortcutLearning/imageCaption.png" width="200px" height="200px"/> 
</figure>
</th>
<th style="text-align:center"><figure>
    <img loading="lazy" src="/ShortcutLearning/recognizePneumonia.png" width="200px" height="200px"/> 
</figure>
</th>
<th style="text-align:center"><figure>
    <img loading="lazy" src="/ShortcutLearning/questionAnswer.png" width="200px" height="200px"/> 
</figure>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>Task</em></td>
<td style="text-align:center">Image Captioning</td>
<td style="text-align:center">Recognize Pneumonia</td>
<td style="text-align:center">Answer Question</td>
</tr>
<tr>
<td style="text-align:center"><em><span style="color:#C34A2C;">Problem</span></em></td>
<td style="text-align:center">Describe scene as grazing sheep</td>
<td style="text-align:center">Fails on scans from new hospital</td>
<td style="text-align:center">Changes answer if irrelevant information is added</td>
</tr>
<tr>
<td style="text-align:center"><em>Shortcut</em></td>
<td style="text-align:center">Uses background(hillside) to recognize primary object</td>
<td style="text-align:center">Looks at hospital token, not lungs</td>
<td style="text-align:center">Only looks at last sentence and ignores context</td>
</tr>
</tbody>
</table>
<p>In the above table Deep Neural Nets are often solving problems taking shortcuts instead of learning the core features. This is prevalent but not only limited to the field of deep learning, infact humans and animals also tend to take shortcuts as we saw an example in the starting of Introduction. These shortcuts tend to work well for confined space but do not generalize well in the real world. For e.g. Amazons&rsquo; DNN model made hiring decision on basis of resume, but were biased for men.</p>
<p>This has given rise to the new field of study in Deep Learning called <b>Shortcut Learning</b>. Shortcuts can be understood as spurious features that perform well on standard benchmarks but fail to generalize to real-world settings[<a href="#2">2</a>]. <em>Spurious features</em> are features that were used during training(e.g. grass) of predictive model e.g classification model but are not useful in general real-world settings(e.g. grass does not imply prediction of cow).</p>
<h2 id="related-works">Related Works</h2>
<p>The field of shortcut learning is not novel problem, it been there for long time in the field of machine learning with different names</p>
<ol>
<li>Learning under Covariate Shift</li>
<li>Anti-Casual Learning</li>
<li>Dataset Bias</li>
<li>Clever Hans Effect</li>
</ol>
<p>Shortcut learning works like an umbrella term to consolidate all the above terms and likewise under the hood to work on the problem as a whole.</p>
<p>Lets&rsquo; start understanding shortcut learning from the biological perspective first.</p>
<h2 id="shortcut-learning-in-biological-neural-networks">Shortcut Learning in Biological Neural Networks</h2>
<h3 id="unintended-cue-learning">Unintended Cue Learning</h3>
<p>&ldquo;<em>In experimental psychology, unexpected failures are often the consequence of unintended cue learning. For example, rats trained to perform a colour discrimination experiment may appear to have learned the task but fail unexpectedly once the odour of the colour paint is controlled for, revealing that they exploited an unintended cue&mdash;smell&mdash;to solve what was intended to be a vision experiment&hellip;<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></em>&rdquo;</p>
<p>Researchers intended the problem to be of vision but rats found the solution by learning the smell of the colors which was unintended by researchers. Though solution worked for given experiment but it will be hard to extrapolate the learnings to other experiments.</p>
<h3 id="surface-learning">Surface Learning</h3>
<p>Students&rsquo; learning process has been broadly classified into two groups 1. Deep Learning and 2. Surface Learning. Taking the reference of Bob and Alice example, Alice would be deep learner and Bob would be the Surface Learner. Bob was focused on grasping the main points and memorizing them, on the other hand Alice showed interest in the meaning of the topics to deepen the understanding by cross linking the learnings with other knowledge. This aid deep learners to transfer their learning strategy to other topics of study or not just subject matter but also to real life dealings. Analogy to machine learning can be, Transfer Learning. Bobs&rsquo; transfer learning is highly intended to fail on other topics unlike Alices'.</p>
<h2 id="example-showing-the-decision-rules-of-shortcut-learning">Example showing the decision rules of Shortcut Learning</h2>
<p>Any machine learning or deep learning model apply decision rules in order to categorize or predict or do any such tasks on a dataset. This defines a relationship between input and output of the model. In this section we will take the example from [<a href="#1">1</a>]</p>
<figure>
    <img loading="lazy" src="/ShortcutLearning/toyExample.png"/> 
</figure>

<p>In the above figure, we are given a dataset containing two categories(star ‚≠ê , crescent üåô) with labels A, B respectively. In the machine learning we usually divide the dataset into training set and test set in some proportion say 80:20 or if the number of datapoints are large then 90:10. Also our datasets usually follow some distribution class wise e.g images of cow would be mainly found in grassfields. Now after dividing the dataset into train and test sets we get i.i.d which is identically independent distribution, think of it as a subset sampled from the dataset. Such that this subset follows the same distribution as dataset. But when only some part of dataset is taken in consideration to make a subset e.g. cow is taken as object of interest, but we change the conditions in which those objects are existing e.g. cow on road instead on grassland. Then this is o.o.d which is out of distribution.</p>
<p>After the model has been trained on the train set, its been tested on i.i.d subset where location of classes seemed to have not changed. Because of which Neural Network model were able to classify them correcty. But, why location? why not shape?. For that we used o.o.d subset in which we changed the location of classes and neural network model failed to classify it correctly. If it would be learning the classification on the basis of shapes which is <em>intended feature</em> then it would have answerd correctly. Wrong classification shows that it learned the location of classes in order to classify which is <em>spurious feature</em>.</p>
<h2 id="where-do-these-shortcuts-come-from">Where do these shortcuts come from?</h2>
<h3 id="principle-of-least-effort">Principle of Least Effort</h3>
<p>Phenomenon of &ldquo;<em>Principle of Least Effort</em>&rdquo; came from the field of lingustics where speakers tend to minimize the amount of effort involved in communication. E.g. speaking &ldquo;plane&rdquo; instead of &ldquo;airplane&rdquo;, merging &ldquo;p&rdquo; and &ldquo;b&rdquo; in &ldquo;cupboard&rdquo;. Related analysis can be found in <a href="#morgans-canon-for-machine-learning">Morgans&rsquo; Canon</a>, <a href="#monitoring-early-training-dynamics">Monitoring Early Training Dynamics</a>.</p>
<h3 id="skewed-training-dataset">Skewed Training Dataset</h3>
<p>Today we mostly use crowd-sourcing platforms like Amazon Mechanical Turk for labeling in vision data, annotating in language data etc for its low cost and scalability. But this comes with its&rsquo; own price of biases and artifacts due to collection bias. Since labelling and annotating datapoints are not done by single entity, this entails individual biases from annotators. For e.g. in NLI task, annotators have usually written contradiction by using <span style="color:#40B5AD"><em>not</em></span> and this created a spurious feature for language model to base its predictions on. Now these annotations are not wrong, but we need robust models to handle this type of bias.</p>
<h3 id="neural-network-models">Neural Network Models</h3>
<p>In LLMs its been seen that two factors are important in order to determine the models&rsquo; robustness. (1) Model Size (2) Pre-training objective<br/></p>
<h4 id="model-size">Model Size</h4>
<p>Model size is measured by number of parameters while keeping the kind of architecture and pre-training objective same. Now generalization ability is measured by the performance of model on o.o.d dataset. Its been shown that generalization ability of larger models are better than those of smaller ones, also smaller models are prone to capture spurious features and more dependent on data artifacts for prediction [<a href="#5">5</a>]. For e.g. BERT-large will generalize better than BERT-base. Following this we can also talk about <b>model compression</b>, a theoritical perspective has shed some light by showing that there is a tradeoff between size of model and robustness[<a href="7">7</a>]. Empirically its also been found that compressing the LLM reduces the robustness and especially the models that have been compressed using knowledge distillation are more vulnerable to adversarial attacks.</p>
<h4 id="pre-training-objectives">Pre-training Objectives</h4>
<p>Lets&rsquo; take 3 kinds of LLMs: BERT, ELECTRA, and RoBERTa,. Now for Adversarial NLI dataset, ELECTRA and RoBERTa have better performance than BERT. Similarly, its&rsquo; been shown that RoBERTa-base outperformed BERT-base on HANS test by 20%. Also empirical evidence show that above three models have different levels of robustness most probably because of inductive bias as pre-training objective.</p>
<h3 id="dataset-bias">Dataset Bias</h3>
<p>Lets&rsquo; take an example of penguin. What makes penguin a penguin? Penguin itself or the context, ofcourse penguin itself but context helps for increasing the probability of correctness. But if only context becomes more important than object itself, that&rsquo;s a problem. If we have penguin in snow data mostly, then strategy of classifying snow as penguin becomes successful even when penguin would not be there. Indeed many models base their predictions on context than object itself and this creates shortcut opportunity for models.</p>
<p>To deal with these dataset biases researchers proposed to scale up the dataset size in order to have sufficient samples with more diversification. Consequently even large real world datasets like Imagenet are highly influenced by it [<a href="#4">4</a>]</p>
<h3 id="discriminative-learning">Discriminative Learning</h3>
<p>Generally it is sufficient for discriminative model to rely on textures and local structures for object classification. But in generative modelling model needs to understand the global shape of object also and not just textures for generating human understandable images. Model is better off learning the textures and ignoring the shape bias for learning object classification task and that becomes a problem e.g. dog image may have the texture of elephant but humans would be classfying the image on the basis of shape rather than texture.</p>
<p>Standard DNNs are intended to learn <em>some</em> workable relationship between input and output and not explicitly take the human interpretability factor of the following relationship into consideration. This ease of finding relationship severly bias the model to learn overly simplistic solutions which perform well on standard benchmarks but fail in real world[<a href="#1">1</a>].</p>
<h2 id="shortcut-learning-across-deep-learning">Shortcut Learning across Deep Learning</h2>
<h3 id="computer-vision">Computer Vision</h3>
<p>Slight changes in the translation, rotation, adding noise, changing background of the object in the image has shown to derail the predictions of DNNs. Models learn shortcuts that are inherent to the distribution of the dataset because of which these models do not perform well when used as transfer learning. Since they have learned the representations which will only work in limited capability under <em>distribution shift</em>. Tiny invisible changes to human eye in the images can alter the predictions of neural network by great margin. This is an example of <em>Adversarial Learning</em> which is one of the most severe failure cases of neural networks[<a href="#1">1</a>]</p>
<h3 id="natural-language-processing">Natural Language Processing</h3>
<p>Studies have found that Large Language Models (LLMs) are vulnerable to adversarial attacks and have low generalization power[<a href="#5">5</a>]. Empirical Analysis shows that BERT-like models base their performance of Natural Language Inference (NLI) task on some spurious features like unigrams <span style="color:#40B5AD"><em>not</em></span>, <span style="color:#40B5AD"><em>do</em></span>, <span style="color:#40B5AD"><em>is</em></span> and bigrams <span style="color:#40B5AD"><em>will not</em></span>. Some models were relying on lexical matching of words between question and original passage in reading comprehension task, which tend to ignore the understanding of comprehension. Natural Language Understanding (NLU) dataset contains artifacts and biases, due to which LLMs using training strategy of Empirical Risk Minimization (ERM) have learned to rely on spurious correlations of them and class labels.</p>
<p>Now features learned by the model can be broadly classified into three categories: (1) Useless Features (2) Non-robust Features (3) Robust Features. Shortcut Learning can also be understood as a phenomenon that rely highly on non-robust features. These type of features are aligned with biases in training data. Below figure may help in providing better understanding at it.</p>
<figure>
    <img loading="lazy" src="/ShortcutLearning/featureGroups.png"/> 
</figure>

<ol>
<li>
<p><cite>Lexical Bias</cite>: Some lexical features like stop words, numbers, negation words have high correlation of co-occurence with certain class labels. E.g. LLMs tend to give contradiction predictions whenever there exist negation words e.g. <span style="color:#40B5AD"><em>never</em></span>, <span style="color:#40B5AD"><em>no</em></span> in input</p>
</li>
<li>
<p><cite>Overlap Bias</cite>: Reading comprehension models use the overlap/lexical matching between passage and question pair for prediction rather than understanding underlying task. Similarly, Question Answering (QA) models perform by relying on heuristics of question and context(knowledge base) lexical matching.</p>
</li>
<li>
<p><cite>Position Bias</cite>: Lets understand this using an example, take the QA task where answers to most of the questions lie in the k<sup>th</sup> line in each passage. This will make QA model to use position cues as spurious feature for prediction.</p>
</li>
<li>
<p><cite>Style Bias</cite>: LLMs have leared to rely on spurious text styles as shortcuts. These features can be further utilized as backdoor attacks for adversarial pertubations to decrease models&rsquo; robustness.</p>
</li>
</ol>
<h3 id="fairness-and-algorithmic-decision-making">Fairness and Algorithmic Decision-Making</h3>
<p>Amazon trained a deep neural network to filter out strong candidates for a job on the basis of resumes, but later it was found out to be gender biased towards men. Gender was such a strong predictor that even after removal of that attribute from dataset, model always found a way around, e.g. inferring gender from all-woman college names. When human biases are not only replicated, but worsened by a machine, this is referred to as <b>bias amplification</b>[<a href="#1">1</a>]. Amplification of disparity among groups over time is referred to as <b>disparity amplification</b>. Example regarding latter can be a shortcut feature that focus on majority group in dataset than underrepresnted.</p>
<h2 id="identification-of-shortcut-learning">Identification of Shortcut Learning</h2>
<h3 id="comprehensive-performance-testing">Comprehensive Performance Testing</h3>
<ol>
<li>
<p>We need to test models on o.o.d datasets to understand if the model has learned the features that we are interested in. HANS (Heuristic Analysis) evaluation set is proposed to evaluate whether NLI models have <b>syntatic heuristics</b>: the <em>lexical overlap heuristic</em>, the <em>subsequence heuristic</em>, the <em>constituent heuristic</em>. Following the philosophy of HANS, a new task has been created: FEVER for facts verification[<a href="#5">5</a>]. These o.o.d tests have shown performance degradation for State-of-the-art Large Language Models.</p>
</li>
<li>
<p>Adversarial Attacks makes an interesting diagnostic tool. If a successful and innocuous adversarial attack can change the predictions of model without changing semantic content, then this is indicative of presence of a shortcut.</p>
</li>
<li>
<p>Ablation studies are useful in order to understand what essential factors are contributing to models performance. Recent ablation studies show that word order does not matter for pre-trained language models. This points out to the fact that LLMs success relies on the ability to understand the correlation between the word co-occurences no matter where words lie in a sentence.</p>
</li>
</ol>
<h3 id="explainability-analysis">Explainability Analysis</h3>
<p>Deep Neural Networks are black boxes which employs a limitation to understand the decision process. Explainability methods are helpful in those scenarios in order to provide some useful insights into the working of DNNs.</p>
<h4 id="feature-attribution">Feature Attribution</h4>
<ol>
<li>
<p>Assume you have an input \( x \) for NLP task, then \( x_{i}\) implies each token. Feature Attribution algorithm \(\Phi\) will calculate \(\Phi_{i}\) which denotes the contribution score of \(x_{i}\) for model prediction.</p>
</li>
<li>
<p>The tokens in the training set can be modeled using long-tail distribution. This can create a shortcut for LLM to concentrate biasedly towards the head of the distribution even though tail of the distribution has abundant information.</p>
</li>
</ol>
<h4 id="instance-attribution">Instance Attribution</h4>
<blockquote>
<p>You are what you eat</p>
<p><cite>&ndash;Victor H. Lindlahr</cite></p>
</blockquote>
<p>The following quote also applies to Deep Neural Networks except they consume data and then base their prediction. Every preceding datapoint affect the prediction of sucessive sample in dataset. Instance Attribution methods are useful in predicting the data samples that contributed more to the models&rsquo; prediction performance for particular instance/datapoint. Empirical analysis indicates that the most influential training data share similar artifacts e.g. high overlapping bias between <em>premise</em> and <em>hypothesis</em> for NLI task.</p>
<h3 id="morgans-canon-for-machine-learning">Morgans&rsquo; Canon for Machine Learning</h3>
<div class="Definition">
    <strong>üìñ Anthromorphism</strong> </br>
    <em>The tendency of humans to attribute human-like psychological characterstics to non-humans on the basis of insufficient empirical evidence</em>
</div>
<p>Psychologist Lloyd Morgon developed a guideline for interpreting non-human behaviour in response to the fallacy of Anthromorphism.</p>
<p><b>Morgons&rsquo; Canon</b> says that &ldquo;<em>In no case is an animal activity to be interpreted in terms of higher psychological processes if it can be fairly interpreted in terms of processes which stand lower on the scale of psychological evolution and development</em>&rdquo;</p>
<p>Simple correlation would be considered low on psychological scale than scene understanding. Morgons&rsquo; canon in terms of machine learning can then be &ldquo;<em>Never attribute to high-level abilities that which can be adequately explained by shortcut learning</em>&rdquo;</p>
<p>Morgons&rsquo; canon can also be understood as Occams&rsquo; Razor that says &ldquo;<em>It is futile to do with more what can be done with fewer</em>&rdquo;. A small patch added to image at certain location for all the images of the class is enough for the model to base its predictions and, complex objects and textures of say some animal would not even be needed for it to classify.</p>
<h3 id="monitoring-early-training-dynamics">Monitoring Early Training dynamics</h3>
<p>Up until now, community has viewed shortcut learning as a distribution shift problem, but [<a href="#2">2</a>] formalized the guideline of Morgons&rsquo; Canon to show difficulty of spurious feature is equally important in order to understand or identify shortcuts. They argued that it can be captured by monitoring early training dynamics, think of monitoring training dynamics as observing the changes of neural network predictions, maybe weights or any likewise insight during the course of training.</p>
<p>Premises that support the <cite>hypothesis</cite>: &ldquo;Difficulty of spurious features is important for shortcut learning&rdquo;</p>
<ol>
<li>Shortcuts are only those spurious features that are <em>easier</em> to learn than the intended features.</li>
<li>Initial layers of DNN learn easy features or low level features, whereas later layers tend to learn harder ones or high level features.</li>
<li>Easy features are learned much earlier than the harder ones during training [<a href="#6">6</a>]</li>
</ol>
<p>lets solve the premises using propositional logic, for ease we will use \(premise_{n} \rightarrow p_{n}\)</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">\(p_{1}: S\)</td>
<td style="text-align:center">\(p_{2}: I \land L\)</td>
<td style="text-align:center">\(p_{3}: E \land H\)</td>
</tr>
<tr>
<td style="text-align:center">\(S:\) &ldquo;Shortcuts are easy features&rdquo;</td>
<td style="text-align:center">\(I: \) &ldquo;Initial layers learn easy features&rdquo;</td>
<td style="text-align:center">\(E: \) &ldquo;Easy features are learned earlier during training&rdquo;</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">\(L: \) &ldquo;Later layers learn hard features&rdquo;</td>
<td style="text-align:center">\(H: \) &ldquo;Hard features are learned later during training&rdquo;</td>
</tr>
</tbody>
</table>
<p>conjuction of \(p_{2}, p_{3}\) \(= p_{2} \land p_{3} = I \land L \land E \land H = (I \land E) \land (L \land H)\)<br/>
we are only concerned with easy features (\(I\)) rather than hard features, since spurious features are easier to learn. So \(L \land H\) can be removed from above equation.<br/></p>
<p>then, \(p_{1} \land p_{2} \land p_{3} = S \land I \land E\)<br/>
This leads to conjecture, &ldquo;<em>Shortcuts are easy features that are learned by initial layers early during training</em>&rdquo;</p>
<p>Now we need to define the notion of task difficulty(\(\Psi\)) which actually depends on model and data distribution (\(X, y\)) where \(X\) is input and \(y\) is label. Then \(\Psi_{M}^{P}(X \rightarrow y)\) indicates difficulty of predicting label \(y\) given input \(X\) for model \(M\) and some distribution \(P\), where \(X, y \sim P\). Spurious feature \(s\) is potential shortcut for model \(M\) iff \(\Psi_{M}^{P_{tr}}(X \rightarrow y) &gt; \Psi_{M}^{P_{tr}}(X \rightarrow s)\). Which means, difficulty of predicting spurious feature \(s\) is easier than predicting label \(y\) for model \(M\) for a training distribution \(P_{tr}\).</p>
<div class="Important">
	<strong>‚ùóWhat does it mean to predict spurious features?</strong> </br>
	Labels are the true predictions for an input, and model can learn to output these labels either by learning intended features or spurious features. So predicting spurious features means correctly predicting the input using spurious features.
</div>
<p>But how would we measure the task difficulty? using two metrics <cite>Prediction Depth</cite> and \(\nu\)-<cite>Usable Information</cite>. <b>Prediction Depth</b> of an input is <em>defined as minimum number of layers required by the model to classify the input</em>. \(\mathit{\boldsymbol{\nu}}\)<b>-Usable Information</b> is defined as <em>amount of usable information in some input </em> \(X\)<em> that can be used by model to predict</em> \(y\), higher the usable information easier the dataset or datapoint/sample is.</p>
<p>We are close to the end of this section, hold tight. Now consider two datasets \(D_{s} \sim P_{tr}(X, y)\) with spurious features and \(D_{i} \sim P_{te}(X, y)\) without spurious features. Taking some assumptions in Prediction Depth (PD) if mean PD of \(D_{s}\) is less than mean PD of \(D_{i}\) then \(\nu\)-Usable Information for \(D_{s}\) is higher than for \(D_{i}\). In such scenerio model will tend to learn spurious features than intended ones. And this can help to identify spurious features.</p>
<figure>
    <img loading="lazy" src="/ShortcutLearning/pdPlot.png"
         alt="Datasets lying on the mean PD axis in order of difficulty. KMNIST with shortcut has least difficulty due to the introduction of shortcut whereas KMNIST with no shorcut is even difficult than FMNIST"/> <figcaption>
            <p>Datasets lying on the mean PD axis in order of difficulty. KMNIST with shortcut has least difficulty due to the introduction of shortcut whereas KMNIST with no shorcut is even difficult than FMNIST</p>
        </figcaption>
</figure>

<h2 id="mitigation-of-shortcut-learning">Mitigation of Shortcut Learning</h2>
<h3 id="dataset-refinement">Dataset Refinement</h3>
<p>The aim of dataset refinement is alleviating biases from training datasets. There are some ways using which we can alleviate biases.</p>
<ol>
<li>
<p>By providing additional instructions to crowd workers to drop down the use of words that are highly indicative of annotation artifacts.</p>
</li>
<li>
<p>Using <cite>Adversarial Filtering</cite> to filter out the bias in dataset. Models trained on these debiased datasets have to learn more generalizable features and rely on common sense reasoning.</p>
</li>
<li>
<p>Using data augmentation methods like counterfactuals, MixUp, CutPaste, rotation and many more can be helpful in debiasing.</p>
</li>
</ol>
<h3 id="adversarial-training">Adversarial Training</h3>
<p>This is implemented in two ways:</p>
<ol>
<li>
<p>Task classifier and adversarial classifier jointly share the same encoder and the goal of the adversarial classifier is to provide correct predictions for the artifacts in training data.</p>
</li>
<li>
<p>Model is trained to minimize the loss function over generated adverarial examples but such techniques haven&rsquo;t shown much generalization abilities.</p>
</li>
</ol>
<p>Many techniques like \(l_{1}\), \(l_{2}\) regularization, CutOut, MixUp and early stopping have been investigated, where <b>early stopping</b> is found to be most effective.</p>
<h3 id="product-of-expertpoe">Product-of-Expert(POE)</h3>
<p>This provides us with an opportunity to model high dimensional data by combining some low dimensional models, where every low dimensional model will focus on some particular constraint of the problem by giving high probability. Since our goal is to debias the model, we can follow two stage system to achieve this.</p>
<ol>
<li>In first stage, bias-only model is trained to capture bias of the dataset.</li>
<li>In second stage, debiased model will be trained using cross-entropy loss and only the weights of debiased model will be updated.</li>
</ol>
<p>In this way debiased model will use the information from biased model to improve its predictions.</p>
<h3 id="training-samples-reweighting">Training Samples Reweighting</h3>
<p>This is also known as <b>worst group loss minimization</b>. The main idea of the following technique is to assign higher weight to hard training examples in a batch. This is because model tend to learn easy features more than hard ones and so improving the performance of worst group (hard samples) is beneficial for model robustness. [<a href="#8">8</a>] used a combination of <b>Low Capacity Network (LCN)</b>, and <b>High Capacity Network (HCN)</b> for reweighting the data samples.</p>
<figure>
    <img loading="lazy" src="/ShortcutLearning/reweighting.png" width="600px" height="500px"/> 
</figure>

<p>We know easy features are learned in early layers of neural network according to Prediction Depth Analysis we did above. LCN can then very well capture those features by been trained to fit to the dataset such that it will give high probabilities for spurious or here easy featues. These probabilities can then be used to reweight the samples that are hard such that model will focus more on hard samples to find relevant and hopefully intended features to learn underlying distribution of the dataset. <b>Imbalance Dataset</b> problem can also be part of this section which researchers of Meta tried to solve by introducing <cite>focal loss</b>.</p>
<h3 id="contrastive-self-supervised-learning">Contrastive Self-Supervised Learning</h3>
<p>Contrastive Learning comes as a subset to Representation Learning which aims to learn better representations by creating auxillary tasks that eventually help the model to perform better on main task or say downstream tasks like classification. Constrastive Learning methods usually suffers with <cite>feature suppression</cite>, say you have two images of one is dog running on a sunny day and another is cat standing still under shade. Model will discriminate both on the basis of <em>sunny</em> and <em>shady</em> supressing other intended features. So auxillary tasks need to created very carefully keeping problems of feature suppression in account.</p>
<h3 id="introducing-challenging-evaluation-datasets">Introducing Challenging Evaluation Datasets</h3>
<p><b>Adversarial GLUE</b> is proposed for adversarial robustness evaluation which contains 14 adversarial attack methods. Similary <b>Checklist</b>, <b>Robustness Gym</b> can be used to evaluate the robustness of LLMs. Introduction to datasets that contain wide range of tasks, biases and so on can give better insights about Deep Neural Nets performance. [<a href="#4">4</a>] introduced <b>Spurious ImageNet</b> which consists of 40 spurious features and each consists 75 images, images didn&rsquo;t had any class objects like car, animal, object but just spurious features. This allowed to check the influence of unintended features over the performance for any vision model.</p>
<h2 id="conclusion">Conclusion</h2>
<blockquote>
<p>Every shortcut has a price usually greater than the reward</p>
<p><cite>&ndash;Bryant H. McGill</cite></p>
</blockquote>
<p>Applications powered using Deep Learning are taking world by storm but under the light of aid that these models are providing us, we shall not unlook the fundamental working principle of these models. Shortcut Learning rather being taken as another problem in the field of Neural Nets, it should be considered default for performance comparision and to better understand what our neural net is actually learning. Overcoming shortcut learning will help in solving fairness, robustness, deployability and trust worthiness of these models. This will eventually make machine decisions more transparent.</p>
<h2 id="citation">Citation</h2>
<p>Cited as:</p>
<blockquote>
<p>P. Garg, ‚ÄúShortcut Learning: Belief trap of Deep Neural Networks,‚Äù Eulogs, Mar. 31, 2023. <a href="https://www.eulogs.com/posts/shortcut-learning/">https://www.eulogs.com/posts/shortcut-learning/</a> (accessed Apr. 13, 2023).</p>
</blockquote>
<p>or</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bibtex" data-lang="bibtex"><span class="line"><span class="cl"><span class="nc">@article</span><span class="p">{</span><span class="nl">garg_2023</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">   <span class="na">title</span>   <span class="p">=</span> <span class="s">&#34;Shortcut Learning: Belief trap of Deep Neural Networks&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">author</span>  <span class="p">=</span> <span class="s">&#34;Garg, Priyam&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">journal</span> <span class="p">=</span> <span class="s">&#34;www.eulogs.com&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">year</span>    <span class="p">=</span> <span class="s">&#34;2023&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">month</span>   <span class="p">=</span> <span class="s">&#34;Mar&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">url</span>     <span class="p">=</span> <span class="s">&#34;https://www.eulogs.com/posts/shortcut-learning/&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="references">References</h2>
<ol>
	<li id="1"> 
		<a href="https://doi.org/10.1038/s42256-020-00257-z">
			Geirhos, Robert, et al. "Shortcut learning in deep neural networks." Nature Machine Intelligence 2.11 (2020): 665-673
		</a>
	</li>
	<li id="2">
		<a href="https://arxiv.org/abs/2302.09344">
			Murali, Nihal, et al. Shortcut Learning Through the Lens of Early Training Dynamics. 1, arXiv, 2023, doi:10.48550/ARXIV.2302.09344
		</a>
	</li>
	<li id="3">
		<a href="https://doi.org/10.1167/jov.20.11.652">
			Geirhos, Robert, et al. ‚ÄúUnintended Cue Learning: Lessons for Deep Learning from Experimental Psychology.‚Äù Journal of Vision, vol. 20, no. 11, Association for Research in Vision and Ophthalmology (ARVO), 20 Oct. 2020, p. 652. Crossref, doi:10.1167/jov.20.11.652.
		</a>
	</li>
	<li id="4">
		<a href="https://doi.org/10.48550/arXiv.2212.04871">
			Neuhaus, Yannic, et al. Spurious Features Everywhere -- Large-Scale Detection of Harmful Spurious Features in ImageNet. 1, arXiv, 2022, doi:10.48550/ARXIV.2212.04871. 
		</a>
	</li>
	<li id="5">
		<a href="https://doi.org/10.48550/arXiv.2208.11857">
			Du, Mengnan, et al. Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey. 1, arXiv, 2022, doi:10.48550/ARXIV.2208.11857. 
		</a>
	</li>
	<li id="6">
		<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=5Lck_J0AAAAJ&citation_for_view=5Lck_J0AAAAJ:j8SEvjWlNXcC">
			Karttikeya Mangalam and Vinay Uday Prabhu. Do deep neural networks learn shallow learnable examples first? 2019.
		</a>
	</li>
	<li id="7">
		<a href="https://doi.org/10.48550/arXiv.2105.12806">
			Bubeck, S√©bastien, and Mark Sellke. A Universal Law of Robustness via Isoperimetry. 4, arXiv, 2021, doi:10.48550/ARXIV.2105.12806. 
		</a>
	</li>
	<li id="8">
		<a href="https://doi.org/10.1016/j.patrec.2022.12.010">
			Dagaev, Nikolay, et al. ‚ÄúA Too-Good-to-Be-True Prior to Reduce Shortcut Reliance.‚Äù Pattern Recognition Letters, vol. 166, Elsevier BV, Feb. 2023, pp. 164‚Äì171. Crossref, doi:10.1016/j.patrec.2022.12.010. 
		</a>
	</li>
</ol>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://doi.org/10.1167/jov.20.11.652">https://doi.org/10.1167/jov.20.11.652</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Conscious AI</title>
      <link>uildDrafts/posts/conscious-ai/</link>
      <pubDate>Tue, 21 Mar 2023 21:29:46 +0530</pubDate>
      
      <guid>uildDrafts/posts/conscious-ai/</guid>
      <description>Summarized explanation of Conscious-AI Research Paper</description>
      <content:encoded><![CDATA[<style>
div {
  margin-bottom: 15px;
  padding: 4px 12px;
}
.Example {
    background-color: #DCDCDC;
    border-left: 6px solid #696969;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Theoram {
        background-color: #E9FFDB;
        border-left: 6px solid #4C9A2A;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Definition {
        background-color: #e1f1fd;
        border-left: 6px solid #72A0C1;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Important{
        background-color: #FFFFE0;
        border-left:6px solid #FFDF00;
        color: #41424C;
        box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Problem {
    background-color: #ffdddd;
    border-left: 6px solid #f44336;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.Info {
    background-color: #e7f3fe;
    border-left: 6px solid #2196F3;
    color: #41424C;
    box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
</style>
<p>DOI: <a href="https://arxiv.org/abs/2105.07879">Paper Link</a></p>
<h1 id="introduction">Introduction</h1>
 <p style="text-align:center;"><b>Three levels of Intelligence</b></p>
<ul>
<li>
<p><strong>Mechanical</strong>
Corresponds to repetitive tasks that require consistency and accuracy, e.g. order-taking machines in restaurants or robots in manufacturing assembly processes.</p>
</li>
<li>
<p><strong>Analytical</strong>
Corresponds to less routine task, but the one which is more inclined towards classification side (e.g., credit application determinations, market segmentation, revenue predictions, etc.)</p>
</li>
<li>
<p><strong>Empathy, Intution and Creativity</strong>
Few AI applications exist at this level. Although empathy and intution are <em>believed</em> to be directly related to human consciousness.</p>
</li>
</ul>
<p>Progression of AI into higher intelligence task can fundamentally disrupt the service industry and severely affect employment and business models as AI agents will replace more humans.</p>
<h1 id="goals-of-paper">Goals of Paper</h1>
<ul>
<li>
<p>Focus on what is required for such conscious state to arise and how we can recognize conscious machines.</p>
</li>
<li>
<p>Putting forward a theory on how consciousness would emerge in its primitive state in AI agents</p>
<ul>
<li>with that, it will also put forward how conscious AI may progress towards a point that we can <em>deterministically</em> recognize it as conscious AI.</li>
</ul>
</li>
<li>
<p>Advancing our understanding of Empathic AI as final stage of intelligence.</p>
</li>
</ul>
<h1 id="preliminary-observation">Preliminary Observation</h1>
<ul>
<li>
<p>Turing (1950) introduced a test that later became known as the Turing Test to pinpoint when it can be said a machine (a standard computational machine) is capable of thinking.</p>
</li>
<li>
<p>The Turing Test can identify <em>thinking machines</em> only at their maturity when they have achieved linguistic indistinguishability from humans</p>
</li>
<li>
<p>Research on consciousness is vast and involves many scientific disciplines (e.g. sociology, neurology, psychology, pathology, philosophy, physics etc)</p>
</li>
</ul>
<h1 id="proposition">Proposition</h1>
<ul>
<li>
<p>Principled framework that can identify <em>thinking machines</em> through their path toward maturity along with minimum requirements under which machine consciousness can emerge.</p>
</li>
<li>
<p>Consciousness in AI is <strong>emergent</strong> phenomenon that manifests when two machines co-create their own language through which they can communicate their internal state.</p>
</li>
</ul>
<h1 id="intelligent-machines">Intelligent Machines</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Weak AI</th>
<th style="text-align:center">Strong AI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Information-processing machines that appear to possess full range of human cognitive abilities</td>
<td style="text-align:center">Defined as intelligent machines that possess all mental and comparable physical capabilities of humans, including consciousness using advanced computation</td>
</tr>
</tbody>
</table>
<p>Authors argue that a form of Strong AI with an emergent consciousness is possible</p>
<h1 id="consciousness-theories">Consciousness Theories</h1>
<blockquote>
<p>Theories focusing on mind, and it&rsquo;s states</p>
</blockquote>
<h2 id="philo-psychological-theories-of-consciousness">Philo-Psychological theories of Consciousness</h2>
<p>Primarily concerned about what consciousness is and how it comes to be within a conscious entity (human).</p>
<p>Entities that play significant role in philo-psychological theories</p>
<ul>
<li>
<p>structure of mind</p>
</li>
<li>
<p>mental states</p>
</li>
<li>
<p>way information is processed, retrieved and stored.</p>
</li>
</ul>
<p>Main focus on <code>internal processes</code> while acknowledging the entity&rsquo;s environment as a source of stimuli.</p>
<h3 id="representationalism">Representationalism</h3>
<p>&nbsp;</p>



<div class="goat svg-container ">
  
    <svg
      xmlns="http://www.w3.org/2000/svg"
      font-family="Menlo,Lucida Console,monospace"
      
        viewBox="0 0 488 121"
      >
      <g transform='translate(8,16)'>
<path d='M 40,0 L 64,0' fill='none' stroke='currentColor'></path>
<path d='M 160,32 L 176,32' fill='none' stroke='currentColor'></path>
<path d='M 336,32 L 360,32' fill='none' stroke='currentColor'></path>
<path d='M 40,64 L 56,64' fill='none' stroke='currentColor'></path>
<path d='M 128,64 L 144,64' fill='none' stroke='currentColor'></path>
<path d='M 320,96 L 344,96' fill='none' stroke='currentColor'></path>
<polygon points='64.000000,64.000000 52.000000,58.400002 52.000000,69.599998' fill='currentColor' transform='rotate(0.000000, 56.000000, 64.000000)'></polygon>
<polygon points='72.000000,0.000000 60.000000,-5.600000 60.000000,5.600000' fill='currentColor' transform='rotate(0.000000, 64.000000, 0.000000)'></polygon>
<polygon points='152.000000,64.000000 140.000000,58.400002 140.000000,69.599998' fill='currentColor' transform='rotate(0.000000, 144.000000, 64.000000)'></polygon>
<polygon points='184.000000,32.000000 172.000000,26.400000 172.000000,37.599998' fill='currentColor' transform='rotate(0.000000, 176.000000, 32.000000)'></polygon>
<polygon points='352.000000,96.000000 340.000000,90.400002 340.000000,101.599998' fill='currentColor' transform='rotate(0.000000, 344.000000, 96.000000)'></polygon>
<polygon points='368.000000,32.000000 356.000000,26.400000 356.000000,37.599998' fill='currentColor' transform='rotate(0.000000, 360.000000, 32.000000)'></polygon>
<path d='M 40,0 A 16,16 0 0,0 24,16' fill='none' stroke='currentColor'></path>
<path d='M 336,32 A 16,16 0 0,0 320,48' fill='none' stroke='currentColor'></path>
<path d='M 24,48 A 16,16 0 0,0 40,64' fill='none' stroke='currentColor'></path>
<path d='M 304,80 A 16,16 0 0,0 320,96' fill='none' stroke='currentColor'></path>
<text text-anchor='middle' x='0' y='4' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='0' y='20' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='0' y='36' fill='currentColor' style='font-size:1em'>R</text>
<text text-anchor='middle' x='0' y='52' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='0' y='68' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='0' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='0' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='8' y='4' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='8' y='20' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='8' y='36' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='8' y='52' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='8' y='68' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='8' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='8' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='16' y='4' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='16' y='20' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='16' y='36' fill='currentColor' style='font-size:1em'>p</text>
<text text-anchor='middle' x='16' y='52' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='16' y='68' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='16' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='16' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='24' y='36' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='32' y='36' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='32' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='32' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='40' y='36' fill='currentColor' style='font-size:1em'>s</text>
<text text-anchor='middle' x='40' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='40' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='48' y='36' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='48' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='48' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='56' y='36' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='56' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='56' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='64' y='36' fill='currentColor' style='font-size:1em'>t</text>
<text text-anchor='middle' x='64' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='64' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='72' y='36' fill='currentColor' style='font-size:1em'>a</text>
<text text-anchor='middle' x='72' y='68' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='72' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='72' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='80' y='4' fill='currentColor' style='font-size:1em'>[</text>
<text text-anchor='middle' x='80' y='36' fill='currentColor' style='font-size:1em'>t</text>
<text text-anchor='middle' x='80' y='68' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='80' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='80' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='88' y='4' fill='currentColor' style='font-size:1em'>F</text>
<text text-anchor='middle' x='88' y='36' fill='currentColor' style='font-size:1em'>i</text>
<text text-anchor='middle' x='88' y='68' fill='currentColor' style='font-size:1em'>d</text>
<text text-anchor='middle' x='88' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='96' y='4' fill='currentColor' style='font-size:1em'>i</text>
<text text-anchor='middle' x='96' y='36' fill='currentColor' style='font-size:1em'>o</text>
<text text-anchor='middle' x='96' y='68' fill='currentColor' style='font-size:1em'>u</text>
<text text-anchor='middle' x='96' y='84' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='104' y='4' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='104' y='36' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='104' y='68' fill='currentColor' style='font-size:1em'>c</text>
<text text-anchor='middle' x='112' y='4' fill='currentColor' style='font-size:1em'>s</text>
<text text-anchor='middle' x='112' y='36' fill='currentColor' style='font-size:1em'>a</text>
<text text-anchor='middle' x='112' y='68' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='120' y='4' fill='currentColor' style='font-size:1em'>t</text>
<text text-anchor='middle' x='120' y='36' fill='currentColor' style='font-size:1em'>l</text>
<text text-anchor='middle' x='128' y='36' fill='currentColor' style='font-size:1em'>i</text>
<text text-anchor='middle' x='136' y='4' fill='currentColor' style='font-size:1em'>O</text>
<text text-anchor='middle' x='136' y='36' fill='currentColor' style='font-size:1em'>s</text>
<text text-anchor='middle' x='144' y='4' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='144' y='36' fill='currentColor' style='font-size:1em'>m</text>
<text text-anchor='middle' x='152' y='4' fill='currentColor' style='font-size:1em'>d</text>
<text text-anchor='middle' x='160' y='4' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='160' y='68' fill='currentColor' style='font-size:1em'>M</text>
<text text-anchor='middle' x='168' y='4' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='168' y='68' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='176' y='4' fill='currentColor' style='font-size:1em'>]</text>
<text text-anchor='middle' x='176' y='68' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='184' y='68' fill='currentColor' style='font-size:1em'>t</text>
<text text-anchor='middle' x='192' y='36' fill='currentColor' style='font-size:1em'>[</text>
<text text-anchor='middle' x='192' y='68' fill='currentColor' style='font-size:1em'>a</text>
<text text-anchor='middle' x='200' y='36' fill='currentColor' style='font-size:1em'>H</text>
<text text-anchor='middle' x='200' y='68' fill='currentColor' style='font-size:1em'>l</text>
<text text-anchor='middle' x='208' y='36' fill='currentColor' style='font-size:1em'>i</text>
<text text-anchor='middle' x='216' y='36' fill='currentColor' style='font-size:1em'>g</text>
<text text-anchor='middle' x='216' y='68' fill='currentColor' style='font-size:1em'>R</text>
<text text-anchor='middle' x='224' y='36' fill='currentColor' style='font-size:1em'>h</text>
<text text-anchor='middle' x='224' y='68' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='232' y='36' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='232' y='68' fill='currentColor' style='font-size:1em'>p</text>
<text text-anchor='middle' x='240' y='36' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='240' y='68' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='248' y='68' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='256' y='36' fill='currentColor' style='font-size:1em'>O</text>
<text text-anchor='middle' x='256' y='68' fill='currentColor' style='font-size:1em'>s</text>
<text text-anchor='middle' x='264' y='36' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='264' y='68' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='272' y='36' fill='currentColor' style='font-size:1em'>d</text>
<text text-anchor='middle' x='272' y='68' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='280' y='36' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='280' y='68' fill='currentColor' style='font-size:1em'>t</text>
<text text-anchor='middle' x='288' y='36' fill='currentColor' style='font-size:1em'>r</text>
<text text-anchor='middle' x='288' y='68' fill='currentColor' style='font-size:1em'>a</text>
<text text-anchor='middle' x='296' y='36' fill='currentColor' style='font-size:1em'>]</text>
<text text-anchor='middle' x='296' y='68' fill='currentColor' style='font-size:1em'>t</text>
<text text-anchor='middle' x='304' y='68' fill='currentColor' style='font-size:1em'>i</text>
<text text-anchor='middle' x='312' y='68' fill='currentColor' style='font-size:1em'>o</text>
<text text-anchor='middle' x='320' y='68' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='360' y='100' fill='currentColor' style='font-size:1em'>[</text>
<text text-anchor='middle' x='368' y='100' fill='currentColor' style='font-size:1em'>P</text>
<text text-anchor='middle' x='376' y='36' fill='currentColor' style='font-size:1em'>[</text>
<text text-anchor='middle' x='376' y='100' fill='currentColor' style='font-size:1em'>h</text>
<text text-anchor='middle' x='384' y='36' fill='currentColor' style='font-size:1em'>I</text>
<text text-anchor='middle' x='384' y='100' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='392' y='36' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='392' y='100' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='400' y='36' fill='currentColor' style='font-size:1em'>t</text>
<text text-anchor='middle' x='400' y='100' fill='currentColor' style='font-size:1em'>o</text>
<text text-anchor='middle' x='408' y='36' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='408' y='100' fill='currentColor' style='font-size:1em'>m</text>
<text text-anchor='middle' x='416' y='36' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='416' y='100' fill='currentColor' style='font-size:1em'>e</text>
<text text-anchor='middle' x='424' y='36' fill='currentColor' style='font-size:1em'>t</text>
<text text-anchor='middle' x='424' y='100' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='432' y='36' fill='currentColor' style='font-size:1em'>i</text>
<text text-anchor='middle' x='432' y='100' fill='currentColor' style='font-size:1em'>a</text>
<text text-anchor='middle' x='440' y='36' fill='currentColor' style='font-size:1em'>o</text>
<text text-anchor='middle' x='440' y='100' fill='currentColor' style='font-size:1em'>l</text>
<text text-anchor='middle' x='448' y='36' fill='currentColor' style='font-size:1em'>n</text>
<text text-anchor='middle' x='448' y='100' fill='currentColor' style='font-size:1em'>]</text>
<text text-anchor='middle' x='456' y='36' fill='currentColor' style='font-size:1em'>a</text>
<text text-anchor='middle' x='456' y='100' fill='currentColor' style='font-size:1em'>	</text>
<text text-anchor='middle' x='464' y='36' fill='currentColor' style='font-size:1em'>l</text>
<text text-anchor='middle' x='472' y='36' fill='currentColor' style='font-size:1em'>]</text>
</g>

    </svg>
  
</div>
<p>&nbsp;</p>
<p>Philosophical idea of representationalism reduces consciousness to <em>mental representations</em> of objects in environment such as photos, signs, natural objects and their qualities.</p>
<table>
<thead>
<tr>
<th style="text-align:center">Intentional</th>
<th style="text-align:center">Phenomenal</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Mental State when it is about, or directed at some object</td>
<td style="text-align:center">Feeling of what it&rsquo;s like to be you</td>
</tr>
<tr>
<td style="text-align:center">e.g. Belief that Earth is round, thought about laptop, perception of animal</td>
<td style="text-align:center">e.g. Perceptual experiences, pains, emotional feelings, episodes of mental imagery, deja vu</td>
</tr>
</tbody>
</table>
<p><span style="color:#008080;"><b>NOTE</b></span>: Most conscious experiences contain both Mental Representations</p>
<h4 id="first-order-representationalism">First-Order Representationalism</h4>
<ul>
<li>
<p>Core idea is that any conscious state is a  <em>representation</em> and what it&rsquo;s like to be in a conscious state is entirely determined by the <em>content</em> of that representation</p>
</li>
<li>
<p>A representation is about something, and the content of that representation is what the representation is about</p>
</li>
<li>
<p>E.g. word <code>DOLPHINS</code> (<em>representation</em>) is about <code>dolphins</code> (<em>content</em>)</p>
<p>&nbsp;</p>
<p style="text-align:center;"><b>Three Classifications are in Order</b></p>
</li>
</ul>
<ol>
<li>
<p><strong>Though a representation has content, a representation is not identical to its content</strong></p>
<p>The representation <code>DOLPHINS</code> is an English word with eight letters, but its content <code>dolphins</code> does not have any letters. Conversely, dolphins swim, but the word <code>DOLPHINS</code> does not swim.</p>
</li>
<li>
<p><strong>The content of a representation can be false, and can concern a non-existent thing</strong></p>
<ul>
<li>
<p>The story of <em>Snow White</em> is about someone who does not exist, but younger children sometimes find it hard to distinguish between the <em>reality</em> and <em>fantasy</em></p>
</li>
<li>
<p>According to representationalists, this explains why illusions, dreams, and hallucinations are possible</p>
</li>
<li>
<p>Consciousness can misrepresent the world</p>
</li>
</ul>
</li>
<li>
<p><strong>First-Order Representationalism does not hold that every contentful representation is conscious</strong></p>
<p>Conscious Representation must be poised to interact directly with one‚Äôs beliefs and desires</p>
</li>
</ol>
<h4 id="higher-order-representationalism">Higher-Order Representationalism</h4>
<ul>
<li>
<p>Address the shortcomings of first-order representationalism by differentiating betweeen conscious and unconscious mental states.</p>
</li>
<li>
<p>It says, mental state is only considered conscious when another mental state within same conscious entity is aware of it.</p>
<ul>
<li>
<p>For example, entity&rsquo;s desire to express opinion becomes conscious only when entity is aware of such a desire.</p>
</li>
<li>
<p>Tongue in person&rsquo;s mouth is aware of it and shows desires and sends signals to brain if it needs to taste something sweet or so.</p>
</li>
</ul>
</li>
</ul>
<h3 id="observations">Observations</h3>
<ul>
<li>
<p>These theories do not offer external observers a direct way of knowing whether they are dealing with conscious entity.</p>
</li>
<li>
<p>Based on these theories, conscious entity may itself know that it is consciouss, but external observer (e.g. human) would not be able to know until that conscious entity inform itself.</p>
</li>
</ul>
<blockquote>
<p>Theory is needed that can potentially offer external observers a way to examine and understand whether an entity (e.g. AI agent) can be considered conscious</p>
</blockquote>
<h2 id="social-self-theory-of-consciousness">Social-Self Theory of Consciousness</h2>
<ul>
<li>
<p>Social-Self theory of consciousness does not interpret consciousness as an individual phenomenon rather a <em>social phenomenon</em>.</p>
</li>
<li>
<p>Following theory focus on <em>individual acts</em> within a <em>social context</em></p>
</li>
<li>
<p>Environment must exist within which actors communicate with each other.</p>
</li>
</ul>
<h2 id="nagels-conceptualization-of-consciousness">Nagels&rsquo; Conceptualization of Consciousness</h2>
<ul>
<li>
<p>Organism is conscious when it knows <em>what it is like to be another organism</em></p>
</li>
<li>
<p>Thus, other conscious organisms must exist within the conscious entity&rsquo;s immediate environment to make it possible for conscious organism to experience what it like to be the other.</p>
</li>
</ul>
<h3 id="symbols">Symbols</h3>
<p><span style="color:#50C878;"><b>EXAMPLE</b></span>: One can assume that a person intends harm if that person approaches with clenched fist. Victim will defend itself from imminent attack while initiator will respond to defenders&rsquo; action. This back-and-forth exchange of symbols constitutes a matrix of social acts or a <strong>social matrix</strong></p>
<p><code>Clenched fist</code> would be considered symbol that carries same meaning for all involved actors.</p>
<p>Consciousness in social-self theory requires a social matrix consisting of social acts and the exchange of symbols that lead to creation of language.</p>
<h2 id="observations-1">Observations</h2>
<ul>
<li>
<p>Theory not concerned with mental processess, existence of mind, internal state which is assumed that all actors bear implictly.</p>
</li>
<li>
<p>Entity might know it is conscious, but external observer might not until conscious entity informs the observer.</p>
</li>
</ul>
<p>None of them (Philo-Psychological and Social-Self theory) alone can provide guidance in positively determining whether an entity is conscious.</p>
<blockquote>
<p>Authors put forward the idea that co-creation of the language is one of the missing links to consciousness</p>
</blockquote>
<h1 id="theories-of-ai-consciousness">Theories of AI Consciousness</h1>
<p>Authors provide 6 propositions regarding AI Consciousness which might help us to formulate or discover a conscious AI agent.</p>
<ol>
<li>
<p><strong>For consciousness to emerge, two AI agents capable of communicating with each other in a shared environment must exist.</strong></p>
<ul>
<li>Mead(1934) stated that language in the form of <em>vocal symbols</em> provides the mechanism for the <em>emergence of consciousness</em>.</li>
<li>He meant exchange of vocal symbols through social acts in a social matrix</li>
<li>So a theory to percieve the emergence of consciousness would be <em>the inception and development of a language</em> among AI Agents.</li>
<li>Language is a means of social interaction and a social phenomenon and so <strong>cannot</strong> be created in isolation but need atleast 2 AI Agents.</li>
<li>We should also focus on the fact that communicating machines already exist, but they ain&rsquo;t conscious. Such that <strong>communication</strong> becomes fundamental to our theory.</li>
</ul>
</li>
<li>
<p><strong>For consciousness to emerge, AI agents must exchange novel signals</strong></p>
<ul>
<li>To infer emergence as a property of an existing system, one must observe <em>something new, a fresh creation</em> that emerges from system instead of being the result of the system&rsquo;s working</li>
<li><em><strong>Creation</strong></em>: Spontaneous idea that appears without much deliberation instead of creativity inherent in deliberate problem-solving activities.</li>
<li>These <em>fresh creations</em> should covey shared meanings among AI Agents.</li>
</ul>
</li>
<li>
<p><strong>For consciousness to emerge, AI agents must turn novel signals into symbols.</strong></p>
<ul>
<li>We need more than a mere exchange of <em>novel signals</em>(fresh creations), but a <strong>shared meaning</strong> for independent onlookers to observe it.</li>
<li><em><strong>Symbols</strong></em>: Novel signals with shared meaning. These symbols are going to be building block of an AI-specific language.</li>
<li>For instance, the object <code>tree</code> is <code>arbor</code> in Latin language and <code>–¥–µ—Ä–µ–≤–æ</code> in Russian. Any of the following words has no material advantage over one another. What&rsquo;s important is AI agents have <em>agreed</em> to use word X for object <code>tree</code></li>
<li>Following is the first step of turning a signal to symbol by providing it with shared meaning.</li>
<li>Meaning arises from agreement, and not from symbol itself. For such agreement to reach, AI Agent must have internal state.</li>
</ul>
</li>
<li>
<p><strong>For consciousness to emerge, AI agents must have internal state</strong></p>
<ul>
<li>To infer a symbol of shared meaning among agents, sender should be aware and understand the meaning of that symbol as the reciever percieves it in a clear cut manner without vagueness or ambiguity.</li>
<li>For a conscious entity to know what it is like to be the other, it must have an internal state in which it can <em>reconstruct</em> the meanings of other AI agent reponses.</li>
</ul>
</li>
<li>
<p><strong>For consciousness to emerge, AI agents must communicate their internal state of time-varying symbol manipulation through a language that they have co-created.</strong></p>
<ul>
<li>Three stages of development in the AI agents&rsquo; path towards consciousness</li>
</ul>
<ol>
<li>
<p><strong>Creation of what in human language we say as noun</strong></p>
<ul>
<li>Two agents shoud agree on <em>random</em> signal to represent a static (time-invariant) the one not varying in time object in their environment</li>
<li>Once such an agreement is reached, the signal is turned into a symbol and must be moved into AI agents&rsquo; <em>permanent memory</em> to be used in future to refer to same static object</li>
<li>e.g. signal X has become symbol for <code>car</code>.</li>
<li>This process creates a symbol what we say in human language as <em>noun</em></li>
</ul>
</li>
<li>
<p><strong>Creation of what in human language we say as verb</strong></p>
<ul>
<li>two agents should agree on a random signal or set of previously created symbols to represent a dynamic (time-variant) concept related to an object in their environment.</li>
<li>e.g. <em>decaying apple</em>, <em>snoring cat</em></li>
<li>Those symbols should be able to describe the changing state of an object.</li>
<li>This process creates a symbol what we say in human language as <em>noun</em></li>
<li>This process is similar to what in human language we say as verbs.</li>
</ul>
</li>
<li>
<p><strong>Creation of new symbols by manipulation in real-time</strong></p>
<ul>
<li>Two Agents should use a set of previously created symbols or a mixture of old symbols and novel signals to express their time varying internal state of <em>symbol manipulation</em></li>
<li>In this final stage, agents will alo communicate their own internal states and how they manipulate symbols in real-time to create new symbols and their associated meanings.</li>
<li>According to authors once the final/third stage is observed, we can conclude consciousness has emerged in Agents.</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>For the emergence of consciousness to be concluded, an onlooker should be able to observe two agents reaching an agreement about atleast one of their state of time-varying symbol manipulaion</strong></p>
<ul>
<li>In order for onlookers to conclude that we are observing conscious AI agents, we need to detect communications about their internal state and how those state <em>change over time</em>.</li>
<li>To detect agents&rsquo; communication about their internal state, author propose that independent onlookers should recognize an explicit(clearly understood) agreement about the meaning of the communication</li>
<li><em>e.g.</em> Two agents cooperatively completing a task they are not programmed to do. Completing a task in such a manner can point to active agreements in the communication of intent and time-varying internal states between agents</li>
</ul>
</li>
</ol>
<h1 id="service-implications">Service Implications</h1>
<ul>
<li>
<p>With the Conscious AI in pursuit, we would need to necessitate new laws and bring forth concepts such as <em>AI ethics</em> and <em>rights</em>.</p>
</li>
<li>
<p>Empathic AI may affect the job market for humans, especially at the higher end of the job market in the service industry.</p>
</li>
<li>
<p>Empathic AI will also have the ability to change the human-AI relationship as people come to trust AI advice and actions, even in hedonic tasks, over advice and actions from another human.</p>
</li>
</ul>
<p><span style="color:#008080;"><b>NOTE</b></span>: The theory of mind also assumes <b>empathy</b> to be the most critical indicator of a fully developed human consciousness and also found to be positively and strongly correlated with trust in interpersonal relationships in which people tend to trust each other‚Äôs recommendations and advice</p>
<h1 id="conclusion">Conclusion</h1>
<p>Authors have introduced a theoritical framework to identify the requirements by which consciousness can <em>emerge</em> in AI agents along with another aim for AI research and practice contrary to current dominant paradigm of creating machines that are <em>linguistically indistinguishable</em> from humans. We need more research to develop refined techinical criteria to recognize the signs of emergent AI consciousness.</p>
<h1 id="external-references">External References</h1>
<ol>
<li>Mehta N, Mashour GA. General and specific consciousness: a first-order representationalist approach. <em>Front Psychol</em>. 2013;4:407. Published 2013 Jul 16. doi:10.3389/fpsyg.2013.00407</li>
</ol>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
