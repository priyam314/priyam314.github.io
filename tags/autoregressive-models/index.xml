<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Autoregressive Models on Eulogs</title>
    <link>https://www.eulogs.com/tags/autoregressive-models/</link>
    <description>Recent content in Autoregressive Models on Eulogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 30 Apr 2023 20:26:02 +0530</lastBuildDate><atom:link href="https://www.eulogs.com/tags/autoregressive-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Autoregressive Models: Connecting the Dots</title>
      <link>https://www.eulogs.com/posts/autoregressive-models-connecting-the-dots/</link>
      <pubDate>Sun, 30 Apr 2023 20:26:02 +0530</pubDate>
      
      <guid>https://www.eulogs.com/posts/autoregressive-models-connecting-the-dots/</guid>
      <description>Desc Text.</description>
      <content:encoded><![CDATA[<style>
.admonition {
  padding: 15px;
  margin-bottom: 21px;
  box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
  
}
.admonition .title {
    margin: 0;
    text-transform: uppercase;
    padding-left: 3px;
    border: 1px solid;
    border-style: hidden hidden solid;
    font-weight: bold;
}
.admonition .content {
    padding-left: .75em;
    padding-right: .75em;
    padding-top: 0.8em;
    margin-left: 0;
    border-left: 0;
    border-top: 0;
    min-height: 0;
    font-size: 1em;
    font-family: ubuntu;
    color: #595652;
}
.Example {
    background-color: #DCDCDC;
    border-left: 10px solid #696969;
}
.Example .title{
    color: #696969;
}
.Theoram {
    background-color: #E9FFDB;
    border-left: 10px solid #4C9A2A;
}
.Theoram .title{
    color: #4C9A2A;
}
.Definition {
    background-color: #e1f1fd;
    border-left: 10px solid #72A0C1;
}
.Definition .title {
    color: #72a0c1;
}
.Important{
    background-color: #FFFFE0;
    border-left:10px solid #FFDF00;
}
.Important .title{
    color: #FFDF00;
}
.Problem {
    background-color: #ffe4e1;
    border-left: 10px solid #CA3433;
}
.Problem .title{
    color: #CA3433;
}
.Info {
    background-color: #e7f3fe;
    border-left: 10px solid #2196F3;
}
.Info .title{
    color: #2196F3;
}
.Question {
    background-color: #FAF0E6;
    border-left: 10px solid #8b814c;
}
.Question .title{
    color: #8b814c;
}
</style>











  
  
  
  





  


<blockquote>
  <p>The future will soon be a thing of past</p>
  <footer>
    <strong>George Carlin</strong>
    
      
        
      
    
  </footer>
</blockquote>

<p>Blogpost will talk about <em>need</em> and <em>what</em> of Autoregressive models, but before going towards the <em>what</em>, its&rsquo; better to first understand why we felt the need to have another type of models in machine learning community. We will look at the Motivation behind the creation and what problems we want to solve using them.</p>
<h2 id="motivation">Motivation</h2>
<h3 id="problems-we-would-like-to-solve">Problems we would like to solve</h3>
<ol>
<li><strong>Generating Data:</strong> syntheizing images, text, videos, speech</li>
<li><strong>Compressing data:</strong> constructing efficient codes.</li>
<li><strong>Anomaly Detection:</strong> e.g. supervised estimator is forced to make a decision even over the datapoint it is uncertain about</li>
</ol>
<p>Suppose there exist true data distribution \(p_{data}(\textbf{x})\), now you most probably will not be able to know this distribution but what you can do is sample some data points from it \(\textbf{x}^{(1)}, \textbf{x}^{(2)} &hellip; \textbf{x}^{(n)} \sim p_{data}(\textbf{x})\). Now likelihood based models try to estimate this \(p_{data}\) by finding a distribution that closely fits to the datapoints sampled. But likelihood function we try to find should basically find probability \(p(\textbf{x})\) for any arbitrary \(\textbf{x}\) such that it will actually closely fits to true distribution and not just sampled distribution, and also be able to sample \(\textbf{x} \sim p(\textbf{x})\)</p>
<p>Its&rsquo; important to note that just because we can sample from distribution so we can compute it too or just because we can compute the distribution then we can sample from it too. But using Autoregressive models we will be able to do both, and actually computing probability will be lot easier than generating samples.</p>
<h3 id="desiderata">Desiderata</h3>
<ol>
<li>
<p>We want to estimate distribution of complex high dimenional data, lets&rsquo; say you have an image of \(128\times 128\times 3\) which will correspond to \(\sim \textbf{50,000}\) pixels(or dimensions). And its&rsquo; just not for one image datapoint but whole lot of dataset.</p>
</li>
<li>
<p>To learn such high dimensional data we need computationl and statistical efficiency. Think of <em>computational efficiency</em> as operation should not take much time and <em>statistical efficiency</em> as it should not take whole lot of datapoints to start understanding the patterns in the datapoints with whatever model we use.</p>
<ul>
<li>Efficient training(computationally and statistically) and model representation</li>
<li>Expressiveness and Generalization</li>
<li>Sampling quality and speed</li>
<li>Compression rate and speed</li>
</ul>
</li>
</ol>
<h2 id="function-approximation">Function Approximation</h2>
<p>Our goal is to estimate true distribution \(p_{data}\) using sampled datapoints \(\textbf{x}^{(1)}, \textbf{x}^{(2)} &hellip; \textbf{x}^{(n)} \sim p_{data}(\textbf{x})\). So we introduce <strong>function approximation</strong> which will learn the weights/parameters \(\theta\) of the likelihood function(here, Neural Network) such that \(p_{\theta}(\textbf{x}) \approx p_{data}(\textbf{x})\).</p>
<div class="admonition Question">
   <div class="title">Question</div>
   <div class="content">
      <ol>
      	<li>
	         How do we design such function approximators to effectively represent complex <b>joint distributions</b> over <b>x</b>, yet remain easy to train?
	      </li>
      </ol>
   </div>
</div>
<p>Now, when we train neural nets what we are actually doing is finding the best weights for the model in order to find best likelihood function. So, designing the model and training procedure go hand in hand. And thi pose a search problem over the parameters.
$$\arg \min_{\theta} \textbf{loss}(\theta, \textbf{x}^{(1)}, &hellip; ,\textbf{x}^{(n)})$$
For intutive understanding, think of likelihood function to be <em>Gaussian Distribution</em> and \(\theta\) be its mean and variance. So, while training \(\theta\) will keep updating in order to fit some distribution.</p>
<p>Most likely objective we use is <em>Maximum Likelihood</em>
$$\arg \min_{\theta} loss(\theta, \textbf{x}^{(1)}, &hellip;, \textbf{x}^{(n)}) = \frac{1}{n} \sum_{i=1}^{n}-\log p_{\theta}(\textbf{x}^{(i)})$$
It is interesting to note that minimizing KL Divergence is asymptotically equivalent to maximizing the likelihood.</p>
<div class="admonition Question">
   <div class = "title">How minimizing KL Divergence asymptotically equivalent to maximizing the likelihood?</div>
   <div class ="content">
   <p>Let there exist likelihood function for true distribution <span
class="math inline">\(P(x\vert\theta^{*})\)</span> and approximate
function <span class="math inline">\(P(x\vert\theta)\)</span> <span
class="math display">\[\begin{aligned}
\theta_{\min KL} &amp;= \arg
\min_{\theta}D_{KL}\left[P(x\vert\theta^{*})\Vert P(x\vert\theta)\right]
\\
                  &amp;= \arg \min_{\theta} \mathbb{E}_{x \sim P(x \vert
\theta^{*})}\left[\log
\frac{P(x\vert\theta^{*})}{P(x\vert\theta)}\right]
\end{aligned}\]</span>  <span
class="math inline">\(P(x\vert\theta^{*})\)</span> is not going to
contribute to the optimization and can be popped out of equation  <span
class="math display">\[\begin{aligned}
\theta_{\min KL} &amp;= \arg \min_{\theta} \mathbb{E}_{x \sim P(x \vert
\theta^{*})}\left[-\log P(x\vert\theta)\right] \\
                  &amp;= \arg \max_{\theta} \mathbb{E}_{x \sim P(x \vert
\theta^{*})}\left[\log P(x\vert\theta)\right]
\end{aligned}\]</span>  Applying the law of large numbers, <span
class="math display">\[\begin{aligned}
\theta_{\min KL} &amp;= \arg \max_{\theta} \lim_{n \rightarrow \infty}
\frac{1}{n}\sum_{i=1}^{n} \log P(x_{i}\vert \theta) \\
                  &amp;= \arg \max_{\theta} \log P(x \vert \theta) \\
                  &amp;= \arg \max_{\theta} P(x \vert \theta) \\
                  &amp;= \theta_{max MLE}
\end{aligned}\]</span></p>
   </div>
</div>
<p>Okay, back to our search problem over parameters we found loss function as maximum likelihood but what procedure to follow for optimization? <strong>Stochastic Gradient Descent</strong> (SGD) seems to work just fine!</p>
<h2 id="setting-bayes-as-base">Setting Bayes as base</h2>
<p>Let us a consider a high-dimensional random variable \(\textbf{x} \in \mathcal{X}^{D}\) where \(\mathcal{X} = {0, 1, &hellip;, 255}\) (e.g. pixel values) or \(\mathcal{X}=\mathbb{R}\). Our goal is to model \(p(\textbf{x})\). For intutive undertanding, say you have an image whose height and width are same as \(\sqrt{D}\) which implies there would be \(D\) pixels or dimensions, each dimension can vary over \(\mathbb{R}\). So to model image \(\textbf{x}\) would be equivalent to model the joint distribution of all the pixels or dimensions of the image.</p>
<p>$$
\begin{aligned}
p(x) &amp;= p(x_{1}, x_{2},&hellip;, x_{d}) \cr
&amp;= p(x_{1})p(x_{2}\vert x_{1})&hellip;p(x_{D}\vert x_{1},&hellip;, x_{D-1}) \cr
&amp;= p(x_{1})\prod_{d=2}^{D}p(x_{d}\vert x_{&lt;d}) \cr
\log p(x) &amp;= \boxed{p(x_{1})\sum_{d=2}^{D}\log p(x_{d}\vert x_{&lt;d})}
\end{aligned}
$$</p>
<p>This is <strong>Autoregressive Model</strong>.</p>
<p>Modelling all conditional distribution separately is simplt infeasible! If we did that we would obtain \(D\) different models, and complexity of each model will grow to varying conditioning. Can we solve this issue? <strong>Yes</strong>, by using single, shared model for the all the conditional distributions and that can be done using neural network as that model.</p>
<h2 id="deep-autoregressive-models-architectures">Deep Autoregressive Models Architectures</h2>
<p>Reearch community has mostly focused on improving the network architectures of deep autoregressive models [4], some areas they focused on are as follows:-</p>
<ul>
<li><strong>Increasing receptive field and memory</strong>: This ensured the network has access to all parts of the input to encourage consistency</li>
<li><strong>Increasing network capacity</strong>: Which allowed more complex distributions to be modelled.</li>
</ul>
<figure>
    <img loading="lazy" src="/AutoregressiveModelling/taxonomy.png"
         alt="Taxonomy of Autoregressive Models" width="800px" height="450px"/> <figcaption>
            <p>Taxonomy of Autoregressive Models</p>
        </figcaption>
</figure>

<p>In the above figure we have tried to do as many plausible classifications of Autoregressive models, there can be more but in the community these are the important ones.</p>
<p>We are no going to learn any of those methods in this blogpost. In future, links to published blogs corresponding to those methods will be added here as list and you can access them directly. This will ease up the navigation process then may it be you want to access root(like in trees) blog post of <a href="https://www.eulogs.com/posts/generative-modelling/#why-generative-modelling">Generative Modelling: Stormbreaker ðŸª“ in AI world</a> series or any leaf blog post like LSTM.</p>
<h2 id="citation">Citation</h2>
<p>Cited as:</p>











  





  


<blockquote>
  <p>Garg, P. (2023, May 2). Autoregressive Models: Connecting the Dots. Eulogs. Retrieved May 2, 2023, from <a href="https://www.eulogs.com/posts/autoregressive-models-connecting-the-dots/">https://www.eulogs.com/posts/autoregressive-models-connecting-the-dots/</a></p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<p>or</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bibtex" data-lang="bibtex"><span class="line"><span class="cl"><span class="nc">@article</span><span class="p">{</span><span class="nl">garg_2023</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">   <span class="na">title</span>   <span class="p">=</span> <span class="s">&#34;Autoregressive Models: Connecting the dots&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">author</span>  <span class="p">=</span> <span class="s">&#34;Garg, Priyam&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">journal</span> <span class="p">=</span> <span class="s">&#34;www.eulogs.com&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">year</span>    <span class="p">=</span> <span class="s">&#34;2023&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">month</span>   <span class="p">=</span> <span class="s">&#34;May&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">url</span>     <span class="p">=</span> <span class="s">&#34;https://www.eulogs.com/posts/autoregressive-models-connecting-the-dots/&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="references">References</h2>
<p>[1] Pieter Abbeel. (2020, February 5). L2 Autoregressive Models &ndash; CS294-158-SP20 Deep Unsupervised Learning &ndash; UC Berkeley, Spring 2020 [Video]. YouTube. <a href="https://www.youtube.com/watch?v=iyEOk8KCRUw">https://www.youtube.com/watch?v=iyEOk8KCRUw</a></p>
<p>[2] Tae, J. (2020a, March 9). Jake Tae. MLE and KL Divergence. <a href="https://jaketae.github.io/study/kl-mle/">https://jaketae.github.io/study/kl-mle/</a></p>
<p>[3] Tomczak, J. M. (2022). Deep Generative Modeling. Springer Nature</p>
<p>[4] Bond-Taylor, S., Leach, A., Long, Y., &amp; Willcocks, C. G. (2021). Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11), 7327â€“7347. <a href="https://doi.org/10.1109/tpami.2021.3116668">https://doi.org/10.1109/tpami.2021.3116668</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Generative Modelling: Stormbreaker ðŸª“ in AI world</title>
      <link>https://www.eulogs.com/posts/generative-modelling/</link>
      <pubDate>Fri, 28 Apr 2023 07:45:46 +0530</pubDate>
      
      <guid>https://www.eulogs.com/posts/generative-modelling/</guid>
      <description>Transformers, Diffusion models, GANs and many such models have taken the world by storm. They all come under the umbrella term of Generative Modelling. In this blog we will see vastness and limitless usage of these models along with their taxonomy.</description>
      <content:encoded><![CDATA[<style>
.admonition {
  padding: 15px;
  margin-bottom: 21px;
  box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
}
.admonition .title {
    margin: 0;
    text-transform: uppercase;
    padding-left: 3px;
    border: 1px solid;
    border-style: hidden hidden solid;
    font-weight: bold;
}
.admonition .content {
    padding-left: .75em;
    padding-right: .75em;
    padding-top: 0.8em;
    margin-left: 0;
    border-left: 0;
    border-top: 0;
    min-height: 0;
    font-size: 1em;
    font-family: ubuntu;
    color: #595652;
}
.Example {
    background-color: #DCDCDC;
    border-left: 10px solid #696969;
}
.Example .title{
    color: #696969;
}
.Theoram {
    background-color: #E9FFDB;
    border-left: 10px solid #4C9A2A;
}
.Theoram .title{
    color: #4C9A2A;
}
.Definition {
    background-color: #e1f1fd;
    border-left: 10px solid #72A0C1;
}
.Definition .title {
    color: #72a0c1;
}
.Important{
    background-color: #fffacd;
    border-left:10px solid #8b864e;
}
.Important .title{
    color: #8b864e;
}
.Problem {
    background-color: #ffe4e1;
    border-left: 10px solid #CA3433;
}
.Problem .title{
    color: #CA3433;
}
.Info {
    background-color: #e7f3fe;
    border-left: 10px solid #2196F3;
}
.Info .title{
    color: #2196F3;
}
.Question {
    background-color: #FAF0E6;
    border-left: 10px solid #8b814c;
}
.Question .title{
    color: #8b814c;
}
</style>
<p>Through this blogpost we are going to gain in-depth understanding of Generative Models, why we need them, along with current taxanomy. But before diving into the <em>Why?</em>, <em>What?</em> of Generative Models, let us consider a simple example of trained Deep Neural Network(DNN) that classifies images of animals. Although the network is trained so well but adding noise to image could output false classification, even though noise added to image doesn&rsquo;t change the semantics under human perception. This example indicates that neural networks that are used to parameterize the conditional distribution \(p(y|\textbf{x})\) seem to lack <em>semantic</em> understanding of images. A useful model in real world will not only output probability of correctness of classification for example but will be able to tell its level of uncertainity for the answer.</p>
<h2 id="why-generative-modelling">Why Generative Modelling?</h2>
<p>Deep Learning models cannot go on making decision about the problems without understanding the reality, and express uncertainity about surrounding world. Assume we have some two-dimensional data and new data-point to be classified. Now, there are two approaches we can use to solve this problem. First, classifier could estimate the result by modelling conditional distribution \(p(y|\textbf{x})\). Second, we can consider joint distribution \(p(\textbf{x}, y)\) that could be further decomposed as \(p(\textbf{x}, y)=p(y|\textbf{x})p(\textbf{x})\).</p>
<figure>
    <img loading="lazy" src="/GenerativeModelling/Discriminative_Generative.png"
         alt="And example of data (left) and two approaches to decision making: (middle) a discriminative approach and (right) a generative approach"/> <figcaption>
            <p>And example of data (left) and two approaches to decision making: (middle) a discriminative approach and (right) a generative approach</p>
        </figcaption>
</figure>

<p>In the above figure, both the approaches i.e. discriminative and generative have made a decision boundary. On one hand former is quite certain of &ldquo;X&rdquo; being a part of blue region whereas latter uses \(p(\textbf{x})\) to account for additional understanding. Now, &ldquo;X&rdquo; lies far from orange zone, but also does not lie in the region of high probability mass in blue zone due to which generative approach said \(p(\textbf{x})\) as low. And, if we want these models to be communicative enough such that as humans we can understand why thy are making decisions that they are making, that would be a <strong>crucial</strong> skill to exploit.</p>
<h3 id="significance-of-span-classmath-inlineempememxemspan">Significance of <span class="math inline"><em>p</em>(<em>x</em>)</span></h3>
<p>From the <em>generative</em> perspective, knowing the distribution \(p(\textbf{x})\) is essential because:</p>
<ul>
<li>It could be used to assess whether a given object has been observed in past or not.</li>
<li>It could help to properly weight the decision.</li>
<li>It could be used to assess uncertainity about the environment.</li>
<li>It could be used to actively learn by interacting with environment( e.g. by asking for labeling objects with low \(p(\textbf{x})\))</li>
<li>It could be used to generate (syntheize) new objects.</li>
</ul>
<p>Though, some if not most of the readers might be ignorant to the use of \(p(\textbf{x})\) only as generator of new data, but above points bring into varied perspectives of it. And at this point, the sail that once started from island of discriminative modelling brought us to the vast land of generative modelling.</p>
<h2 id="where-can-we-use-deep-generative-modelling">Where Can we use (Deep) Generative Modelling?</h2>
<p>Deep Generative Modelling need high computational power for training and with the advent of GPU and Deep Learning Frameworks like PyTorch, TensorFlow etc we have seen vast applications thereof.</p>
<p>Some applications vary from typical modalities considered in machine learning:-</p>
<ol>
<li><strong>Text Analysis:</strong> Question-Answering, Machine Translation, Text-to-Image Generation</li>
<li><strong>Image-Analysis:</strong> Data Augmentation, Super Resolution, Image Inpainting, Image Denoising, Object Transfiguration, Image Colorization, Image Captioning, Video Prediction etc.</li>
<li><strong>Audio Analysis:</strong> WaveNet</li>
<li><strong>Active Learning:</strong> Generate synthetic medical images, generate synthetic text data etc.</li>
<li><strong>Reinforcement Learning:</strong> World Models</li>
<li><strong>Graph Analysis:</strong> Understanding Interaction Dynamics in Social Networks, Anomaly Detection, Protein Structure Modelling, Source Code Generation, Semantic Parsing.</li>
</ol>
<h2 id="how-to-formulate-deep-generative-modelling">How to Formulate (Deep) Generative Modelling?</h2>
<p>After asking some important questions of <em>What</em> and <em>where</em>, its&rsquo; time to ask <em>how</em>. In other words we can understand the section heading as how to express \(p(\textbf{x})\). We can divide (Deep) Generative modelling into four categories:- \((1)\) Autoregressive Generative Models(ARM) \((2)\) Flow-based Models \((3)\) Latent variable models \((4)\) Energy-based models</p>
<figure>
    <img loading="lazy" src="/GenerativeModelling/taxonomy.png"
         alt="A taxonomy of deep generative models" width="700px" height="550px"/> <figcaption>
            <p>A taxonomy of deep generative models</p>
        </figcaption>
</figure>

<p>Deep Generative models can be divided into two types of density estimation methods, namely \((1)\) Explicit Density Models \((2)\) Implicit Density Models. Say you have data distribution \(p_{data}\), \(p_{model}(\textbf{x};\theta)\) likelihood function which learns that data distribution and \(\textbf{x}=[x_{1}, x_{2}, &hellip;, x_{n}]^{T}\) are data-points of dataset. Now in explicit density models we compute \(p_{model}(\textbf{x};\theta)\) in short \(p(\textbf{x})\), but in implicit models we dont&rsquo; need to compute this likelihood function but directly sample from the distribution e.g. sampling values from noise, and generate for instance image samples using some transformation.</p>
<table>
<thead>
<tr>
<th style="text-align:center">Explicit Generative Methods</th>
<th style="text-align:center">Impliit Generative Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">define an explicit density form(\(p_{model}(\textbf{x};\theta)\)) that allows likelihood inference</td>
<td style="text-align:center">target a flexible transformation(Generator) from random noise to generated samples</td>
</tr>
<tr>
<td style="text-align:center">used to estimate the probability density function of the data</td>
<td style="text-align:center">can generate new samples from the learned distribution</td>
</tr>
<tr>
<td style="text-align:center">require computing the likelihood of the data, which can be computationally expensive</td>
<td style="text-align:center">do not require computing the likelihood</td>
</tr>
<tr>
<td style="text-align:center">typically easier to interpret and analyze</td>
<td style="text-align:center">more flexible and can capture complex distributions</td>
</tr>
<tr>
<td style="text-align:center">can suffer from overfitting</td>
<td style="text-align:center">can suffer from mode collapse</td>
</tr>
<tr>
<td style="text-align:center">trained using maximum likelihood estimation</td>
<td style="text-align:center">typically trained using adversarial training or variational inference</td>
</tr>
</tbody>
</table>
<h3 id="explicit-generative-methods">Explicit Generative Methods</h3>
<p>Explicit density methods need to estimate the density of underlying data distribution and marginal likelihood in denominator of bayes rule \(p(z|x)=\frac{p(x,z)}{\int p(x|z)p(z)}\) creates problem since it involves integral. Now if this likelihood can be solved in closed-form expression then those problems lie under <em>tractable density</em> otherwise we need to find approximations to it and so those problems lie under <em>approximate density</em> class. Also, tractable density can be computed in polynomial time but intractable(not approximate) takes more than exponential time to solve. In the given link of <a href="https://en.wikipedia.org/wiki/Closed-form_expression">Closed Form Expressions</a> a table is given where you can check that we do not have any closed form solution under integral.</p>
<h4 id="tractable-generative-models">Tractable Generative Models</h4>
<p>In tractable density models we have two type of techniques namely \((1)\) Autoregressive Models \((2)\) Flow-based Models.</p>
<h5 id="autoregressive-models">Autoregressive Models</h5>
<p>Here, distribution over \(\textbf{x}\) is represented in an autoregressive manner:</p>
<p>$$p(\textbf{x})=p(x_{0})\prod_{i=1}^{D}p(x_{i}\vert\textbf{x}_{&lt;i})$$</p>
<p>where, \(\textbf{x}_{&lt;i}\) denotes all \(\textbf{x}\)&rsquo;s upto index \(i\).</p>
<p>Modelling all conditional distributions \(p(x_{i}\vert\textbf{x}_{&lt;i})\) would be computationally inefficient. But there are many ways we solve this, although we will not talk about autoregessive models here. Checkout the post <a href="https://www.eulogs.com/posts/autoregressive-models-connecting-the-dots/">Autoregressive Models: Connecting the dots</a> that talk about why we needed these type of models anyway along with its taxonomy.</p>
<h5 id="flow-based-models">Flow-based Models</h5>
<p>The change of variable formula provides a principled manner of expressing a density of a random variable by transforming it with an invertible transformation \(f\).</p>
<p>$$p(\textbf{x})=p(z=f(x))\vert\text{J}_{f(\textbf{x})}\vert$$</p>
<p>where \(\text{J}_{f(\textbf{x})}\) denotes the Jacobian Matrix.</p>
<p>We can parameterize \(f\) using deep neural networks; however, it cannot be any arbitrary neural networks, because we must be able to calculate the Jacobian matrix. All generative models that take advantage of change of variables formula are referred as <strong>flow-based models</strong> or <em>flows</em> for short.</p>
<h4 id="approximate-generative-models">Approximate Generative Models</h4>
<p>In approximate density models we have two type of techniques namely \((1)\) Prescribed Models \((2)\) Energy-based Models.</p>
<h5 id="prescribed-models">Prescribed Models</h5>
<p>The idea behind <strong>latent variable models</strong> is to assume a lower-dimensional latent space and the following generative process:
$$\textbf{z} \sim p(\textbf{z})$$
$$\textbf{x} \sim p(\textbf{x}\vert\textbf{z})$$
Latent variables corresponds to hidden factor in data, and the conditional distribution \(p(\textbf{x}\vert\textbf{z})\) could be treated as a <em>generator</em>.</p>
<p>The most widely known known latent variable model is <strong>probabilisitc Principal Component Analysis</strong> (pPCA) where \(p(\textbf{z})\) and \(p(\textbf{x}\vert\textbf{z})\) are Gaussian distibutions, and dependency between \(\textbf{z}\) and \(\textbf{x}\) is linear.</p>
<p>A non-linear extension of pPCA with arbitratry distributions is the <strong>Variational Auto-Encoder</strong> (VAE) framework. In VAEs and the pPCA all distributions must be defined upfront and, therefore, they are called <em>prescribed models</em>.</p>
<h5 id="energy-based-models">Energy Based Models</h5>
<p>Physics provide an interesting perspective on defining a group of generative models through defining an <em>energy function</em>, \(E(\textbf{x})\), and, eventually the Boltzmann distribution:
$$p(\textbf{x}) = \frac{e^{-E(\textbf{x})}}{Z}$$
where \(Z=\sum_{\textbf{x}}e^{-E(\textbf{x})}\) is the partition function. \(Z\) normalizes the values of function.</p>
<p>The main idea behind EBMs is to formulate the energy function and calculate(or rather approximate) the partition function.</p>
<h3 id="implicit-generative-methods">Implicit Generative Methods</h3>
<h4 id="gans">GANs</h4>
<p>Above all described methods except EBMs used <em>log-likelihood</em> function for density estimation, but another approach uses <em>adversarial loss</em> where discriminator \(D(\cdot)\) determines a difference between real data and synthetic data provided by the generator in the implicit form, these are <strong>Generative Adversaial Networks</strong>(GANs) under <em>implicit models</em>.</p>
<div class="admonition Important">
    <div class="title">Important</div>
    <div class="content">
      It is to be noted that these groups dont' create hard demarcations and there are methods that use concepts from more than one groups for e.g. flow-based GAN model
    </div>
</div>
<h2 id="overview">Overview</h2>
<p>Below is provided a table that shows comparision of all the four groups of methods on some arbitrary criteria like:-</p>
<ul>
<li>Whether training is typically <strong>stable</strong>?</li>
<li>Whether it is possible to calculate <strong>likelihood function</strong>?</li>
<li>Whether one can use a model for <strong>lossy</strong> or <strong>lossless</strong> compression?</li>
<li>Whether a model can be used for <strong>Representation Learning</strong>?</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">Generative Models</th>
<th style="text-align:center">Training</th>
<th style="text-align:center">Likelihood</th>
<th style="text-align:center">Sampling</th>
<th style="text-align:center">Compression</th>
<th style="text-align:center">Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Autoregressive</td>
<td style="text-align:center"><span style="color:#03AC13">Stable</span></td>
<td style="text-align:center"><span style="color:#03AC13">Exact</span></td>
<td style="text-align:center"><span style="color:#B90E0A">Slow</span></td>
<td style="text-align:center"><span style="color:#03AC13">Lossless</span></td>
<td style="text-align:center"><span style="color:#B90E0A">No</span></td>
</tr>
<tr>
<td style="text-align:left">Flow-based</td>
<td style="text-align:center"><span style="color:#03AC13">Stable</span></td>
<td style="text-align:center"><span style="color:#03AC13">Exact</span></td>
<td style="text-align:center"><span style="color:#03AC13">Fast</span>/<span style="color:#B90E0A">Slow</span></td>
<td style="text-align:center"><span style="color:#03AC13">Lossless</span></td>
<td style="text-align:center"><span style="color:#03AC13">Yes</span></td>
</tr>
<tr>
<td style="text-align:left">Implicit</td>
<td style="text-align:center"><span style="color:#B90E0A">Unstable</span></td>
<td style="text-align:center"><span style="color:#B90E0A">No</span></td>
<td style="text-align:center"><span style="color:#03AC13">Fast</span></td>
<td style="text-align:center">No</td>
<td style="text-align:center"><span style="color:#B90E0A">No</span></td>
</tr>
<tr>
<td style="text-align:left">Prescribed</td>
<td style="text-align:center"><span style="color:#03AC13">Stable</span></td>
<td style="text-align:center"><span style="color:#03AC13">Approximate</span></td>
<td style="text-align:center"><span style="color:#03AC13">Fast</span></td>
<td style="text-align:center"><span style="color:#B90E0A">Lossy</span></td>
<td style="text-align:center"><span style="color:#03AC13">Yes</span></td>
</tr>
<tr>
<td style="text-align:left">Energy-based</td>
<td style="text-align:center"><span style="color:#03AC13">Stable</span></td>
<td style="text-align:center"><span style="color:#B90E0A">Unnormalized</span></td>
<td style="text-align:center"><span style="color:#B90E0A">Slow</span></td>
<td style="text-align:center">Rather not</td>
<td style="text-align:center"><span style="color:#03AC13">Yes</span></td>
</tr>
</tbody>
</table>
<h2 id="citation">Citation</h2>











  





  


<blockquote>
  <p>Garg, P. (2023a, April 30). Generative Modelling: Stormbreaker ðŸª“ in AI world. Eulogs. Retrieved May 2, 2023, from <a href="https://www.eulogs.com/posts/generative-modelling/">https://www.eulogs.com/posts/generative-modelling/</a></p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<p>or</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bibtex" data-lang="bibtex"><span class="line"><span class="cl"><span class="nc">@article</span><span class="p">{</span><span class="nl">garg_2023</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">title</span>   <span class="p">=</span> <span class="s">&#34;Generative Modelling: Stormbreaker ðŸª“ in AI world&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">author</span>  <span class="p">=</span> <span class="s">&#34;Garg, Priyam&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">journal</span> <span class="p">=</span> <span class="s">&#34;www.eulogs.com&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">year</span>    <span class="p">=</span> <span class="s">&#34;2023&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">month</span>   <span class="p">=</span> <span class="s">&#34;Apr&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="na">url</span>     <span class="p">=</span> <span class="s">&#34;https://www.eulogs.com/posts/generative-modelling/&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="references">References</h2>
<p>[1] Tomczak, J. M. (2022). Deep Generative Modeling. Springer Nature</p>
<p>[2] Wu, Q., Gao, R., &amp; Zha, H. (2021). Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators. In Neural Information Processing Systems (Vol. 34). <a href="https://papers.nips.cc/paper/2021/hash/5db60c98209913790e4fcce4597ee37c-Abstract.html">https://papers.nips.cc/paper/2021/hash/5db60c98209913790e4fcce4597ee37c-Abstract.html</a></p>
<p>[3] Bond-Taylor, S., Leach, A., Long, Y., &amp; Willcocks, C. G. (2021). Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11), 7327â€“7347. <a href="https://doi.org/10.1109/tpami.2021.3116668">https://doi.org/10.1109/tpami.2021.3116668</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
